<!-- doc/src/xml/high-availability.xml -->

<chapter id="high-availability">
 <title>Отказоустойчивость, балансировка нагрузки и репликация</title>

 <indexterm><primary>отказоустойчивость</primary></indexterm>
 <indexterm><primary>отработка отказа</primary></indexterm>
 <indexterm><primary>репликация</primary></indexterm>
 <indexterm><primary>балансировка нагрузки</primary></indexterm>
 <indexterm><primary>кластеризация</primary></indexterm>
 <indexterm><primary>секционирование данных</primary></indexterm>

 <para>Серверы базы данных могут работать совместно для обеспечения возможности быстрого переключения на другой сервер в случае отказа первого (отказоустойчивость) или для обеспечения возможности нескольким серверам БД обрабатывать один набор данных (балансировка нагрузки). В идеале, серверы БД могут работать вместе прозрачно для клиента. Веб-серверы, обрабатывающие статические страницы, можно совместить достаточно легко посредством простого распределения запросов на несколько машин. Фактически серверы баз данных только для чтения тоже могут быть совмещены достаточно легко. К сожалению, большинство серверов баз данных получают смешанные запросы на чтение/запись, а серверы с доступом на чтение/запись совместить гораздо сложнее. Это объясняется тем, что данные только для чтения достаточно единожды разместить на каждом сервере, а запись на любой из серверов должна распространиться на все остальные серверы, чтобы будущие запросы на чтение возвращали согласованные результаты.</para>

 <para>Проблема синхронизации является главным препятствием для совместной работы серверов. Так как единственного решения, устраняющего проблему синхронизации во всех случаях, не существует, предлагается несколько решений. Разные решения подходят к проблеме по-разному и минимизируют её влияние в разных рабочих условиях.</para>

 <para>Некоторые решения применяют синхронизацию, позволяя только одному серверу изменять данные. Сервер, который может изменять данные, называется сервером чтения/записи, <firstterm>ведущим</firstterm> или <firstterm>главным</firstterm> сервером. Сервер, который отслеживает изменения на ведущем, называется <firstterm>ведомым</firstterm> или <firstterm>резервным</firstterm> сервером. Резервный сервер, к которому нельзя подключаться до тех пор, пока он не будет повышен до главного, называется сервером <firstterm>тёплого резерва</firstterm>, а тот, который может принимать соединения и обрабатывать запросы только на чтение, называется сервером <firstterm>горячего резерва</firstterm>.</para>

 <para>Некоторые решения являются синхронными, при которых транзакция, модифицирующая данные, не считается подтверждённой, пока все серверы не подтвердят транзакцию. Это гарантирует, что при отработке отказа не произойдёт потеря данных и что все балансирующие серверы возвращают целостные данные вне зависимости от того, к какому серверу был запрос. Асинхронное решение, напротив, допускает некоторую задержку между временем подтверждения транзакции и её передачей на другие серверы, допуская возможность, что некоторые транзакции могут быть потеряны в момент переключения на резервный сервер и что балансирующие серверы могут вернуть слегка устаревшие данные. Асинхронная передача используется, когда синхронная будет слишком медленной.</para>

 <para>Решения могут так же разделяться по степени детализации. Некоторые решения работают только на уровне всего сервера БД целиком, в то время как другие позволяют работать на уровне таблиц или уровне БД.</para>

 <para>В любом случае необходимо принимать во внимание быстродействие. Обычно выбирается компромисс между функциональностью и производительностью. Например, полностью синхронное решение в медленной сети может снизить производительность больше чем наполовину, в то время как асинхронное решение будет оказывать минимальное воздействие.</para>

 <para>В продолжении этого раздела рассматриваются различные решения по организации отказоустойчивости, репликации и балансировки нагрузки.</para>

 <sect1 id="different-replication-solutions">
 <title>Сравнение различных решений</title>

 <variablelist>

  <varlistentry>
   <term>Отказоустойчивость на разделяемых дисках</term>
   <listitem>

    <para>Отказоустойчивость на разделяемых дисках позволяет избежать избыточности синхронизации путём задействования только одной копии базы данных. Она использует единственный дисковый массив, который разделяется между несколькими серверами. Если основной сервер БД откажет, резервный сервер может подключиться и запустить базу данных, что позволит восстановить БД после аварии. Это обеспечивает быстрое переключение без потери данных.</para>

    <para>Функциональность разделяемого оборудования обычно реализована в сетевых устройствах хранения. Так же возможно применение сетевой файловой системы; особое внимание следует уделить тому, чтобы поведение системы полностью соответствовало <acronym>POSIX</acronym> (см. <xref remap="4" linkend="creating-cluster-nfs"/>). Существенное ограничение этого метода состоит в том, что в случае отказа или порчи разделяемого дискового массива оба сервера: главный и резервный — станут нерабочими. Другая особенность — резервный сервер никогда не получает доступ к разделяемым дискам во время работы главного.</para>

   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Репликация на уровне файловой системы (блочного устройства)</term>
   <listitem>

    <para>Видоизменённая версия функциональности разделяемого оборудования представлена в виде репликации на уровне файловой системы, когда все изменения в файловой системе отражаются в файловой системе другого компьютера. Единственное ограничение: синхронизация должна выполняться методом, гарантирующим целостность копии файловой системы на резервном сервере &mdash; в частности, запись на резервном сервере должна происходить в том же порядке, что и на главном. <productname>DRBD</productname> является популярным решением на основе репликации файловой системы для Linux.</para>

<!--
https://forge.continuent.org/pipermail/sequoia/2006-November/004070.html

Oracle RAC is a shared disk approach and just send cache invalidations
to other nodes but not actual data. As the disk is shared, data is
only committed once to disk and there is a distributed locking
protocol to make nodes agree on a serializable transactional order.
-->

   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Трансляция журнала транзакций</term>
   <listitem>

    <para>Серверы тёплого и горячего резерва могут так же поддерживаться актуальными путём чтения потока записей из журнала изменений (<acronym>WAL</acronym>). Если основной сервер отказывает, резервный содержит почти все данные с него и может быть быстро преобразован в новый главный сервер БД. Это можно сделать синхронно или асинхронно, но может быть выполнено только на уровне сервера БД целиком.</para>
    <para>Резервный сервер может быть реализован с применением трансляции файлов журналов (см. <xref remap="4" linkend="warm-standby"/>), или потоковой репликации (см. <xref remap="4" linkend="streaming-replication"/>), или их комбинацией. За информацией о горячем резерве обратитесь к <xref remap="3" linkend="hot-standby"/>.</para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Репликация главный-резервный на основе триггеров</term>
   <listitem>

    <para>При репликации главный-резервный все запросы, изменяющие данные, пересылаются главному серверу. Главный сервер, в свою очередь, асинхронно пересылает изменённые данные резервному. Резервный сервер может обрабатывать запросы только на чтение при работающем главном. Такой резервный сервер идеален для обработки запросов к хранилищам данных.</para>

    <para><productname>Slony-I</productname> является примером подобного типа репликации, действующей на уровне таблиц, и поддерживает множество резервных серверов. Так как обновления на резервных серверах происходят асинхронно (в пакетах), возможна потеря данных во время отказа.</para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Репликация запросов в среднем слое</term>
   <listitem>

    <para>В схеме с репликацией запросов в среднем слое, средний слой перехватывает каждый SQL-запрос и пересылает его на один или все серверы. Каждый сервер работает независимо. Модифицирующие запросы должны быть направлены всем серверам, чтобы каждый из них получал все изменения. Но читающие запросы могут быть посланы только на один сервер, что позволяет перераспределить читающую нагрузку между всеми серверами.</para>

    <para>Если запросы просто перенаправлять без изменений, функции подобные <function>random()</function>, <function>CURRENT_TIMESTAMP</function> и последовательности могут получить различные значения на разных серверах. Это происходит потому что каждый сервер работает независимо, а эти запросы неизбирательные (и действительно не изменяют строки). Если такая ситуация недопустима, или средний слой, или приложение должно запросить подобные значения с одного сервера, затем использовать его в других пишущих запросах. Другим способом является применения этого вида репликации совместно с другим традиционным набором репликации главный-резервный, то есть изменяющие данные запросы посылаются только на главный сервер, а затем применяются на резервном в процессе этой репликации, но не с помощью реплицирующего среднего слоя. Следует иметь в виду, что все транзакции фиксируются или прерываются на всех серверах, возможно с применением двухфазной фиксации (см. <xref remap="4" linkend="sql-prepare-transaction"/> и <xref remap="4" linkend="sql-commit-prepared"/>). Репликацию такого типа реализуют, например <productname>Pgpool-II</productname> и <productname>Continuent Tungsten</productname>.</para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Асинхронная репликация с несколькими главными серверами</term>
   <listitem>

    <para>Если серверы не находятся постоянно в единой сети, как например, ноутбуки или удалённые серверы, обеспечение согласованности данных между ними представляет проблему. Когда используется асинхронная репликация с несколькими главными серверами, каждый из них работает независимо и периодически связывается с другими серверами для определения конфликтующих транзакций. Конфликты могут урегулироваться пользователем или по правилам их разрешения. Примером такого типа репликации является Bucardo.</para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Синхронная репликация с несколькими главными серверами</term>
   <listitem>

    <para>При синхронной репликации с несколькими главными серверами каждый сервер может принимать запросы на запись, а изменённые данные передаются с начального сервера всем остальным, прежде чем транзакция будет подтверждена. Если запись производится интенсивно, это может провоцировать избыточные блокировки, что приводит к снижению производительности. На самом деле производительность при записи часто бывает хуже, чем с одним сервером. Запросы на чтение также могут быть обработаны любым сервером. В некоторых конфигурациях для более эффективного взаимодействия серверов применяются разделяемые диски. Синхронная репликация с несколькими главными серверами лучше всего работает, когда преобладают операции чтения, хотя её большой плюс в том, что любой сервер может принимать запросы на запись &mdash; нет необходимости искусственно разделять нагрузку между главным и резервными серверами, а так как изменения передаются от одного сервера другим, не возникает проблем с недетерминированными функциями вроде <function>random()</function>.</para>

    <para><productname>Postgres Pro Enterprise</productname> включает расширение <filename>multimaster</filename>, реализующее этот вид репликации. Используя <filename>multimaster</filename>, вы можете организовать синхронный кластер без разделения ресурсов, который обеспечивает расширяемость <acronym>OLTP</acronym> для читающих транзакций, а также высокую степень доступности с автоматическим восстановлением после сбоев. За подробностями обратитесь к <xref remap="3" linkend="multimaster"/>.</para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Коммерческие решения</term>
   <listitem>

    <para>Так как <productname>PostgreSQL</productname> обладает открытым кодом и легко расширяется, некоторые компании взяли за основу <productname>PostgreSQL</productname> и создали коммерческие решения с закрытым кодом со своими реализациями свойств отказоустойчивости, репликации и балансировки нагрузки.</para>
   </listitem>
  </varlistentry>

 </variablelist>

 <para><xref linkend="high-availability-matrix"/> итоговая таблица возможностей различных решений приведена ниже.</para>

 <table id="high-availability-matrix">
  <title>Таблица свойств отказоустойчивости, балансировки нагрузки и репликации</title>
  <tgroup cols="8">
   <thead>
    <row>
     <entry>Тип</entry>
     <entry>Отказоустойчивость через разделяемые диски</entry>
     <entry>Репликация файловой системы</entry>
     <entry>Трансляция журнала транзакций</entry>
     <entry>Репликация главный-резервный на основе триггеров</entry>
     <entry>Репликация запросов в среднем слое</entry>
     <entry>Асинхронная репликация с несколькими главными серверами</entry>
     <entry>Синхронная репликация с несколькими главными серверами</entry>
    </row>
   </thead>

   <tbody>

    <row>
     <entry>Наиболее типичная реализация</entry>
     <entry align="center">NAS</entry>
     <entry align="center">DRBD</entry>
     <entry align="center">Потоковая репликация</entry>
     <entry align="center">Slony</entry>
     <entry align="center">pgpool-II</entry>
     <entry align="center">Bucardo</entry>
     <entry align="center">расширение <filename>multimaster</filename> в составе <productname>Postgres Pro Enterprise</productname></entry>
    </row>

    <row>
     <entry>Метод взаимодействия</entry>
     <entry align="center">разделяемые диски</entry>
     <entry align="center">дисковые блоки</entry>
     <entry align="center">WAL</entry>
     <entry align="center">Строки таблицы</entry>
     <entry align="center">SQL</entry>
     <entry align="center">Строки таблицы</entry>
     <entry align="center">Строки таблицы и блокировки строк</entry>
    </row>

    <row>
     <entry>Не требуется специального оборудования</entry>
     <entry align="center"/>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Допускается несколько главных серверов</entry>
     <entry align="center"/>
     <entry align="center"/>
     <entry align="center"/>
     <entry align="center"/>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Нет избыточности главного сервера</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"/>
     <entry align="center">&bull;</entry>
     <entry align="center"/>
     <entry align="center">&bull;</entry>
     <entry align="center"/>
     <entry align="center"/>
    </row>

    <row>
     <entry>Нет задержки при нескольких серверах</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"/>
     <entry align="center">без синхр.</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"/>
     <entry align="center">&bull;</entry>
     <entry align="center"/>
    </row>

    <row>
     <entry>Отказ главного сервера не может привести к потере данных</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">с синхр.</entry>
     <entry align="center"/>
     <entry align="center">&bull;</entry>
     <entry align="center"/>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Резервный сервер принимает читающие запросы</entry>
     <entry align="center"/>
     <entry align="center"/>
     <entry align="center">с горячим резервом</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Репликация на уровне таблиц</entry>
     <entry align="center"/>
     <entry align="center"/>
     <entry align="center"/>
     <entry align="center">&bull;</entry>
     <entry align="center"/>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Не требуется разрешение конфликтов</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"/>
     <entry align="center"/>
     <entry align="center">&bull;</entry>
    </row>

   </tbody>
  </tgroup>
 </table>

 <para>Несколько решений, которые не подпадают под указанные выше категории:</para>

 <variablelist>

  <varlistentry>
   <term>Секционирование данных</term>
   <listitem>

    <para>При секционировании таблицы расщепляются на наборы данных. Каждый из наборов может быть изменён только на одном сервере. Например, данные могут быть секционированы по офисам, например, Лондон и Париж, с сервером в каждом офисе. В случае необходимости обращения одновременно к данным Лондона и Парижа, приложение может запросить оба сервера, или может быть применена репликация главный-резервный для предоставления копии только для чтения в другом офисе для каждого из серверов.</para>
   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Выполнение параллельных запросов на нескольких серверах</term>
   <listitem>

    <para>Многие из указанных выше решений позволяют обрабатывать несколько запросов на нескольких серверах, но ни один из них не может обрабатывать один запрос с применением нескольких серверов для уменьшения общего времени выполнения. Подобное решение позволяет нескольким серверам обрабатывать один запрос одновременно. Такое обычно достигается путём разделения данных между серверами, обработкой на сервере своей части запроса с возвратом результата на центральный сервер. Там данные проходят окончательную обработку и возвращаются пользователю. <productname>Pgpool-II</productname> предоставляет такую возможность. Так же это может быть реализовано с применением набора средств <productname>PL/Proxy</productname>.</para>

   </listitem>
  </varlistentry>

 </variablelist>

 </sect1>


 <sect1 id="warm-standby">
 <title>Трансляция журналов на резервные серверы</title>


  <para>Постоянная архивация может использоваться для создания кластерной конфигурации <firstterm>высокой степени доступности</firstterm> (HA) с одним или несколькими <firstterm>резервными серверами</firstterm>, способными заменить ведущий сервер в случае выхода его из строя. Такую реализацию отказоустойчивости часто называют <firstterm>тёплый резерв</firstterm> или <firstterm>трансляция журналов</firstterm>.</para>

  <para>Ведущий и резервный серверы работают совместно для обеспечения этой возможности, при этом они связаны опосредованно. Ведущий сервер работает в режиме постоянной архивации изменений, в то время как каждый резервный сервер работает в режиме постоянного приема архивных изменений, зачитывая WAL-файлы с ведущего. Для обеспечения этой возможности не требуется вносить изменения в таблицы БД, что требует существенно меньших административных издержек в сравнении с некоторыми другими решениями репликации. Так же такая конфигурация относительно слабо влияет на производительность ведущего сервера.</para>

  <para>Непосредственную передачу записей WAL с одного сервера БД на другой обычно называют трансляцией журналов (или доставкой журналов). <productname>&productname;</productname> реализует трансляцию журналов на уровне файлов, передавая записи WAL по одному файлу (сегменту WAL) единовременно. Файлы WAL (размером 16 МБ) можно легко и эффективно передать на любое расстояние, будь то соседний сервер, другая система в местной сети или сервер на другом краю света. Требуемая пропускная способность при таком подходе определяется скоростью записи транзакций на ведущем сервере. Трансляция журналов на уровне записей более фрагментарная операция, при которой изменения WAL передаются последовательно через сетевое соединение (см. <xref remap="4" linkend="streaming-replication"/>).</para>

  <para>Следует отметить, что трансляция журналов асинхронна, то есть записи WAL доставляются после завершения транзакции. В результате образуется окно, когда возможна потеря данных при отказе сервера: будут утеряны ещё не переданные транзакции. Размер этого окна при трансляции файлов журналов может быть ограничен параметром <varname>archive_timeout</varname>, который может принимать значение меньше нескольких секунд. Тем не менее подобные заниженные значения могут потребовать существенного увеличения пропускной способности, необходимой для трансляции файлов. При потоковой репликации (см. <xref remap="4" linkend="streaming-replication"/>) окно возможности потери данных гораздо меньше.</para>

  <para>Скорость восстановления достаточно высока, обычно резервный сервер становится полностью доступным через мгновение после активации. В результате такое решение называется тёплым резервом, что обеспечивает отличную отказоустойчивость. Восстановление сервера из архивной копии базы и применение изменений обычно происходит существенно дольше. Поэтому такие действия обычно требуются при восстановлении после аварии, не для отказоустойчивости. Так же резервный сервер может обрабатывать читающие запросы. В этом случае он называется сервером горячего резерва. См. <xref remap="4" linkend="hot-standby"/> для подробной информации.</para>

  <indexterm zone="high-availability"><primary>тёплый резерв</primary></indexterm>

  <indexterm zone="high-availability"><primary>PITR резерв</primary></indexterm>

  <indexterm zone="high-availability"><primary>резервный сервер</primary></indexterm>

  <indexterm zone="high-availability"><primary>трансляция журналов</primary></indexterm>

  <indexterm zone="high-availability"><primary>следящий сервер</primary></indexterm>

  <indexterm zone="high-availability"><primary>STONITH</primary></indexterm>

  <sect2 id="standby-planning">
   <title>Планирование</title>

   <para>Обычно разумно подбирать ведущий и резервный серверы так, чтобы они были максимально похожи, как минимум с точки зрения базы данных. Тогда в частности, пути, связанные с табличными пространствами, могут передаваться без изменений. Таким образом, как на ведущем, так и на резервных серверах должны быть одинаковые пути монтирования для табличных пространств при использовании этой возможности БД. Учитывайте, что если <xref linkend="sql-createtablespace"/> выполнена на ведущем сервере, новая точка монтирования для этой команды уже должна существовать на резервных серверах до её выполнения. Аппаратная часть не должна быть в точности одинаковой, но опыт показывает, что сопровождать идентичные системы легче, чем две различные на протяжении жизненного цикла приложения и системы. В любом случае архитектура оборудования должна быть одинаковой &mdash; например, трансляция журналов с 32-битной на 64-битную систему не будет работать.</para>

   <para>В общем случае трансляция журналов между серверами с различными основными версиями <productname>&productname;</productname> невозможна. Политика главной группы разработки &productname; состоит в том, чтобы не вносить изменения в дисковые форматы при обновлениях корректирующей версии, таким образом, ведущий и резервный серверы, имеющие разные корректирующие версии, могут работать успешно. Тем не менее, формально такая возможность не поддерживается и рекомендуется поддерживать одинаковую версию ведущего и резервных серверов, насколько это возможно. При обновлении корректирующей версии безопаснее будет в первую очередь обновить резервные серверы &mdash; новая корректирующая версия с большей вероятностью прочитает файл WAL предыдущей корректирующей версии, чем наоборот.</para>

  </sect2>

  <sect2 id="standby-server-operation">
   <title>Работа резервного сервера</title>

   <para>Сервер, работающий в режиме резервного, последовательно применяет файлы WAL, полученные от главного. Резервный сервер может читать файлы WAL из архива WAL (см. <xref remap="4" linkend="restore-command"/>) или напрямую с главного сервера по соединению TCP (потоковая репликация). Резервный сервер также будет пытаться восстановить любой файл WAL, найденный в кластере резервного в каталоге <filename>pg_xlog</filename>. Это обычно происходит после перезапуска сервера, когда он применяет заново файлы WAL, полученные от главного сервера перед перезапуском. Но можно и вручную скопировать файлы в каталог <filename>pg_xlog</filename>, чтобы применить их в любой момент времени.</para>

   <para>В момент запуска резервный сервер начинает восстанавливать все доступные файлы WAL, размещённые в архивном каталоге, указанном в команде <varname>restore_command</varname>. По достижении конца доступных файлов WAL или при сбое команды <varname>restore_command</varname> сервер пытается восстановить все файлы WAL, доступные в каталоге <filename>pg_xlog</filename>. Если это не удаётся и потоковая репликация настроена, резервный сервер пытается присоединиться к ведущему и начать закачивать поток WAL с последней подтверждённой записи, найденной в архиве или <filename>pg_xlog</filename>. Если это действие закончилось неудачей, или потоковая репликация не настроена, или соединение позднее разорвалось, резервный сервер возвращается к шагу 1 и пытается восстановить файлы из архива вновь. Цикл обращения за файлами WAL к архиву, <filename>pg_xlog</filename>, и через потоковую репликацию продолжается до остановки сервера или переключения его роли, вызванного файлом-триггером.</para>

   <para>Режим резерва завершается и сервер переключается в обычный рабочий режим при получении команды <command>pg_ctl promote</command> или при обнаружении файла-триггера (<varname>trigger_file</varname>). Перед переключением сервер восстановит все файлы WAL, непосредственно доступные из архива или <filename>pg_xlog</filename>, но пытаться подключиться к главному серверу он больше не будет.</para>
  </sect2>

  <sect2 id="preparing-master-for-standby">
   <title>Подготовка главного сервера для работы с резервными</title>

   <para>Настройка постоянного архивирования на ведущем сервере в архивный каталог, доступный с резервного, описана в <xref remap="6" linkend="continuous-archiving"/>. Расположение архива должно быть доступно с резервного сервера даже при отключении главного, то есть его следует разместить на резервном или другом доверенном, но не на главном сервере.</para>

   <para>При использовании потоковой репликации следует настроить режим аутентификации на ведущем сервере, чтобы разрешить соединения с резервных. Для этого создать роль и обеспечить подходящую запись в файле <filename>pg_hba.conf</filename> в разделе доступа к БД <literal>replication</literal>. Так же следует убедиться, что для параметра <varname>max_wal_senders</varname> задаётся достаточно большое значение в конфигурационном файле ведущего сервера. При использовании слотов для репликации также достаточно большое значение нужно задать для <varname>max_replication_slots</varname>.</para>

   <para>Создание базовой резервной копии, необходимой для запуска резервного сервера, описано в <xref remap="6" linkend="backup-base-backup"/>.</para>
  </sect2>

  <sect2 id="standby-server-setup">
   <title>Настройка резервного сервера</title>

   <para>Для запуска резервного сервера нужно восстановить резервную копию, снятую с ведущего (см. <xref remap="4" linkend="backup-pitr-recovery"/>). Затем нужно создать файл команд восстановления <filename>recovery.conf</filename> в каталоге данных кластера резервного сервера и включить режим <varname>standby_mode</varname>. Задайте в <varname>restore_command</varname> обычную команду копирования файлов из архива WAL. Если планируется несколько резервных серверов в целях отказоустойчивости, установите для <varname>recovery_target_timeline</varname> значение <literal>latest</literal>, чтобы резервный сервер переходил на новую линию времени, образуемую при отработке отказа и переключении на другой сервер.</para>

   <note>
     <para>Не используйте pg_standby или подобные средства со встроенным режимом резервного сервера, описанным здесь. <varname>restore_command</varname> должна немедленно прекратиться при отсутствии файла; сервер повторит команду вновь при необходимости. Использование средств, подобных pg_standby, описано в <xref remap="6" linkend="log-shipping-alternative"/>.</para>
   </note>

   <para>При необходимости потоковой репликации заполните <varname>primary_conninfo</varname> параметрами строки соединения для libpq, включая имя (или IP-адрес) сервера и все остальные необходимые данные для соединения с ведущим сервером. Если ведущий требует пароль для аутентификации, пароль может быть так же передан в <varname>primary_conninfo</varname>.</para>

   <para>Если резервный сервер настраивается в целях отказоустойчивости, на нём следует настроить архивацию WAL, соединения и аутентификацию, как на ведущем сервере, потому что резервный сервер станет ведущим после отработки отказа.</para>

   <para>При использовании архива WAL его размер может быть уменьшен с помощью команды в параметре <xref linkend="archive-cleanup-command"/>, которая удаляет файлы уже не нужные для дальнейшей работы резервного сервера. Утилита <application>pg_archivecleanup</application> разработана специально для использования в <varname>archive_cleanup_command</varname> при типичной конфигурации с одним резервным сервером (см. <xref remap="4" linkend="pgarchivecleanup"/>). Следует отметить, что если архив используется в целях резервирования, следует сохранять все файлы необходимые для восстановления как минимум с последней базовой резервной копии, даже если они не нужны для резервного сервера.</para>

   <para>Простой пример <filename>recovery.conf</filename>: <programlisting>standby_mode = 'on'
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
restore_command = 'cp /path/to/archive/%f %p'
archive_cleanup_command = 'pg_archivecleanup /path/to/archive %r'</programlisting></para>

   <para>Можно поддерживать любое количество резервных серверов, но при применении потоковой репликации необходимо убедиться, что значение <varname>max_wal_senders</varname> на ведущем достаточно большое, чтобы все они могли подключиться одновременно.</para>

  </sect2>

  <sect2 id="streaming-replication">
   <title>Потоковая репликация</title>

   <indexterm zone="high-availability"><primary>Потоковая репликация</primary></indexterm>

   <para>При потоковой репликации резервный сервер может работать с меньшей задержкой, чем при трансляции файлов. Резервный сервер подключается к ведущему, который передаёт поток записей WAL резервному в момент их добавления, не дожидаясь окончания заполнения файла WAL.</para>

   <para>Потоковая репликация асинхронна по умолчанию (см. <xref remap="4" linkend="synchronous-replication"/>), то есть имеется небольшая задержка между подтверждением транзакции на ведущем сервере и появлением этих изменений на резервном. Тем не менее, эта задержка гораздо меньше, чем при трансляции файлов журналов, обычно в пределах одной секунды, если резервный сервер достаточно мощный и справляется с нагрузкой. При потоковой репликации настраивать <varname>archive_timeout</varname> для уменьшения окна потенциальной потери данных не требуется.</para>

   <para>При потоковой репликации без постоянной архивации на уровне файлов, сервер может избавиться от старых сегментов WAL до того, как резервный получит их. В этом случае резервный сервер потребует повторной инициализации из новой базовой резервной копии. Этого можно избежать, установив для <varname>wal_keep_segments</varname> достаточно большое значение, при котором сегменты WAL будут защищены от ранней очистки, либо настроив слот репликации для резервного сервера. Если с резервного сервера доступен архив WAL, этого не требуется, так как резервный может всегда обратиться к архиву для восполнения пропущенных сегментов.</para>

   <para>Чтобы включить потоковую репликацию, сначала настройте резервный сервер на приём трансляции журналов, как описано в <xref remap="6" linkend="warm-standby"/>. Затем сделайте следующий шаг — переключите резервный сервер в режим репликации, установив в <varname>primary_conninfo</varname> в файле <filename>recovery.conf</filename> строку подключения, указывающую на ведущий. Настройте <xref linkend="guc-listen-addresses"/> и параметры аутентификации (см. <filename>pg_hba.conf</filename>) на ведущем сервере таким образом, чтобы резервный смог подключиться к псевдобазе <literal>replication</literal> на ведущем (см. <xref remap="4" linkend="streaming-replication-authentication"/>).</para>

   <para>В системах, поддерживающих параметр сокета keepalive, подходящие значения <xref linkend="guc-tcp-keepalives-idle"/>, <xref linkend="guc-tcp-keepalives-interval"/> и <xref linkend="guc-tcp-keepalives-count"/> помогут ведущему вовремя заметить разрыв соединения.</para>

   <para>Установите максимальное количество одновременных соединений с резервных серверов (см. описание <xref linkend="guc-max-wal-senders"/>.</para>

   <para>При запуске резервного сервера с правильно установленным <varname>primary_conninfo</varname> резервный подключится к ведущему после воспроизведения всех файлов WAL, доступных из архива. При успешном установлении соединения можно увидеть процесс walreceiver на резервном сервере и соответствующий процесс walsender на ведущем.</para>

   <sect3 id="streaming-replication-authentication">
    <title>Аутентификация</title>
    <para>Право использования репликации очень важно ограничить так, чтобы только доверенные пользователи могли читать поток WAL, так как из него можно извлечь конфиденциальную информацию. Резервный сервер должен аутентифицироваться на ведущем от имени суперпользователя или пользователя с правом <literal>REPLICATION</literal>. Настоятельно рекомендуется создавать выделенного пользователя с правами <literal>REPLICATION</literal> и <literal>LOGIN</literal> специально для репликации. Хотя право <literal>REPLICATION</literal> даёт очень широкие полномочия, оно не позволяет модифицировать данные в ведущей системе, тогда как с правом <literal>SUPERUSER</literal> это можно делать.</para>

    <para>Список аутентификации клиентов для репликации содержится в <filename>pg_hba.conf</filename> в записях с установленным значением <literal>replication</literal> в поле <replaceable>database</replaceable>. Например, если резервный сервер запущен на компьютере с IP-адресом <literal>192.168.1.100</literal> и учётная запись для репликации <literal>foo</literal>, администратор может добавить следующую строку в файл <filename>pg_hba.conf</filename> ведущего: <programlisting># Разрешить пользователю "foo" с компьютера 192.168.1.100 подключаться к этому
# серверу в качестве партнёра репликации, если был передан правильный пароль.
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    replication     foo             192.168.1.100/32        md5</programlisting></para>
    <para>Имя компьютера и номер порта для ведущего, имя пользователя для соединения и пароль указываются в файле <filename>recovery.conf</filename>. Пароль так же может быть задан через файл <filename>~/.pgpass</filename> на резервном сервере (указанном в определении с <literal>replication</literal> в поле <replaceable>database</replaceable>). Например, если ведущий принимает подключения по IP-адресу <literal>192.168.1.50</literal>, в порту <literal>5432</literal>, пользователя для репликации <literal>foo</literal> с паролем <literal>foopass</literal>, то администратор может добавить следующую строку в файл <filename>recovery.conf</filename> на резервном сервере: <programlisting># Резервный сервер подключается к ведущему, работающему на компьютере 192.168.1.50
# (порт 5432), от имени пользователя "foo" с паролем "foopass".
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'</programlisting></para>
   </sect3>

   <sect3 id="streaming-replication-monitoring">
    <title>Наблюдение</title>
    <para>Важным индикатором стабильности работы потоковой репликации является количество записей WAL, созданных на ведущем, но ещё не применённых на резервном сервере. Можно подсчитать задержку, сравнив текущий WAL, записанный на ведущем, с последним WAL, полученным на резервном. Эти показатели могут быть получены с помощью функций <function>pg_current_xlog_location</function> на ведущем и <function>pg_last_xlog_receive_location</function> на резервном соответственно (более подробно см. <xref remap="4" linkend="functions-admin-backup-table"/> и <xref remap="4" linkend="functions-recovery-info-table"/>). Местоположение последнего полученного WAL на резервном сервере так же показывается в статусе процесса получателя WAL, отображаемого по команде <command>ps</command> (более подробно см. <xref remap="4" linkend="monitoring-ps"/>).</para>
    <para>Можно запросить список процессов отправителей WAL через представление <link linkend="monitoring-stats-views-table"><literal>pg_stat_replication</literal></link>. Большая разница между <function>pg_current_xlog_location</function> и полем <literal>sent_location</literal> указывает на то, что главный сервер работает с большой нагрузкой, в то время как разница между <literal>sent_location</literal> и <function>pg_last_xlog_receive_location</function> на резервном указывает на задержки в сети или на то, что с большой нагрузкой работает резервный.</para>
   </sect3>
  </sect2>

  <sect2 id="streaming-replication-slots">
   <title>Слоты репликации</title>
   <indexterm><primary>Слоты репликации</primary> <secondary>Потоковая репликация</secondary></indexterm>
   <para>Слоты репликации автоматически обеспечивают механизм сохранения сегментов WAL, пока они не будут получены всеми резервными и главный сервер не будет удалять строки, находящиеся в статусе <link linkend="hot-standby-conflict">recovery conflict</link> даже при отключении резервного.</para>
   <para>Вместо использования слотов репликации для предотвращения удаления старых сегментов WAL можно применять <xref linkend="guc-wal-keep-segments"/>, или сохранять сегменты в архиве с помощью команды <xref linkend="guc-archive-command"/>. Тем не менее, эти методы часто приводят к тому, что хранится больше сегментов WAL, чем необходимо, в то время как слоты репликации оставляют только то количество сегментов, которое необходимо. Преимущество этих методов состоит в том, что они чётко задают объёмы места, необходимого для <literal>pg_xlog</literal>; в то время как текущая реализация репликационных слотов не представляет такой возможности.</para>
   <para>Подобным образом, параметры <xref linkend="guc-hot-standby-feedback"/> и <xref linkend="guc-vacuum-defer-cleanup-age"/> позволяют защитить востребованные строки от удаления при очистке, но первый параметр не защищает в тот промежуток времени, когда резервный сервер не подключён, а для последнего часто нужно задавать большое значение, чтобы обеспечить должную защиту. Слоты репликации решают эти проблемы.</para>
   <sect3 id="streaming-replication-slots-manipulation">
    <title>Запросы и действия слотов репликации</title>
    <para>Каждый слот репликации обладает именем, состоящим из строчных букв, цифр и символов подчёркивания.</para>
    <para>Имеющиеся слоты репликации и их статус можно просмотреть в представлении <link linkend="view-pg-replication-slots"><structname>pg_replication_slots</structname></link>.</para>
    <para>Слоты могут быть созданы и удалены как с помощью протокола потоковой репликации (см. <xref remap="4" linkend="protocol-replication"/>), так и посредством функций SQL (см. <xref remap="4" linkend="functions-replication"/>).</para>
   </sect3>
   <sect3 id="streaming-replication-slots-config">
    <title>Пример конфигурации</title>
    <para>Для создания слота репликации выполните: <programlisting>postgres=# SELECT * FROM pg_create_physical_replication_slot('node_a_slot');
  slot_name  | xlog_position
-------------+---------------
 node_a_slot |

postgres=# SELECT * FROM pg_replication_slots;
  slot_name  | slot_type | datoid | database | active | xmin | restart_lsn | confirmed_flush_lsn
-------------+-----------+--------+----------+--------+------+-------------+---------------------
 node_a_slot | physical  |        |          | f      |      |             |
(1 row)</programlisting> Для настройки резервного сервера на использование этого слота <varname>primary_slot_name</varname> должно быть настроено в конфигурации <filename>recovery.conf</filename> резервного. Вот простейший пример: <programlisting>standby_mode = 'on'
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
primary_slot_name = 'node_a_slot'</programlisting></para>
   </sect3>
  </sect2>

  <sect2 id="cascading-replication">
   <title>Каскадная репликация</title>

   <indexterm zone="high-availability"><primary>Каскадная репликация</primary></indexterm>

   <para>Свойство каскадной репликации позволяет резервному серверу принимать соединения репликации и потоки WAL от других резервных, выступающих посредниками. Это может быть полезно для уменьшения числа непосредственных подключений к главному серверу, а также для уменьшения накладных расходов при передаче данных в интрасети.</para>

   <para>Резервный сервер, выступающий как получатель и отправитель, называется каскадным резервным сервером. Резервные серверы, стоящие ближе к главному, называются серверами верхнего уровня, а более отдалённые — серверами нижнего уровня. Каскадная репликация не накладывает ограничений на количество или организацию последующих уровней, а каждый резервный соединяется только с одним сервером вышестоящего уровня, который в конце концов соединяется с единственным главным/ведущим сервером.</para>

   <para>Резервный сервер каскадной репликации не только получает записи WAL от главного, но так же восстанавливает их из архива. Таким образом, даже если соединение с сервером более высокого уровня разорвётся, потоковая репликация для последующих уровней будет продолжаться до исчерпания доступных записей WAL.</para>

   <para>Каскадная репликация в текущей реализации асинхронна. Параметры синхронной репликации (см. <xref remap="4" linkend="synchronous-replication"/>) в настоящее время не оказывают влияние на каскадную репликацию.</para>

   <para>Распространение обратной связи горячего резерва работает от нижестоящего уровня к вышестоящему уровню вне зависимости от способа организации связи.</para>

   <para>Если резервный сервер вышестоящего уровня будет преобразован в новый главный, серверы нижестоящего уровня продолжат получать поток с нового главного при условии, что <varname>recovery_target_timeline</varname> установлен в значение <literal>'latest'</literal>.</para>

   <para>Для использования каскадной репликации необходимо настроить резервный каскадный сервер на прием соединений репликации (то есть установить <xref linkend="guc-max-wal-senders"/> и <xref linkend="guc-hot-standby"/>, настроить <link linkend="auth-pg-hba-conf">host-based authentication</link>). Так же может быть необходимо настроить на нижестоящем резервном значение <varname>primary_conninfo</varname> на каскадный резервный сервер.</para>
  </sect2>

  <sect2 id="synchronous-replication">
   <title>Синхронная репликация</title>

   <indexterm zone="high-availability"><primary>Синхронная репликация</primary></indexterm>

   <para>По умолчанию в <productname>&productname;</productname> потоковая репликация асинхронна. Если ведущий сервер выходит из строя, некоторые транзакции, которые были подтверждены, но не переданы на резервный, могут быть потеряны. Объём потерянных данных пропорционален задержке репликации на момент отработки отказа.</para>

   <para>Синхронная репликация предоставляет возможность гарантировать, что все изменения, внесённые в транзакции, были переданы одному или нескольким синхронным резервным серверам. Это увеличивает стандартный уровень надёжности, гарантируемый при фиксации транзакции. Этот уровень защиты соответствует второму уровню безопасности репликации из теории вычислительной техники, или групповой безопасности первого уровня (безопасности групповой и уровня 1), когда выбран режим <varname>synchronous_commit</varname> <literal>remote_write</literal>.</para>

   <para>При синхронной репликации каждая фиксация пишущей транзакции ожидает подтверждения того, что транзакция записана в журнал транзакций на диске на обоих серверах: главном и резервном. При таком варианте потеря данных может произойти только в случае одновременного выхода из строя ведущего и резервного серверов. Это обеспечивает более высокий уровень надёжности, при условии продуманного подхода системного администратора к вопросам размещения и управления этими серверами. Ожидание подтверждения увеличивает уверенность в том, что данные не будут потеряны во время сбоя сервера, но при этом увеличивает время отклика для обработки транзакции. Минимальное время ожидания равно времени передачи данных от ведущего к резервному и обратно.</para>

   <para>Транзакции только для чтения и откат транзакции не требуют ожидания для ответа с резервного сервера. Промежуточные подтверждения не ожидают ответа от резервного сервера, только подтверждение верхнего уровня. Долгие операции вида загрузки данных или построения индекса не ожидают финального подтверждения. Но все двухфазные подтверждения требуют ожидания, включая подготовку и непосредственно подтверждение.</para>

   <sect3 id="synchronous-replication-config">
    <title>Базовая настройка</title>

   <para>При настроенной потоковой репликации установка синхронной репликации требует только дополнительной настройки: необходимо выставить <xref linkend="guc-synchronous-standby-names"/> в непустое значение. Так же необходимо установить <varname>synchronous_commit</varname> в значение <literal>on</literal>, но так как это значение по умолчанию, обычно действий не требуется. (См. <xref remap="4" linkend="runtime-config-wal-settings"/> и <xref remap="4" linkend="runtime-config-replication-master"/>.) В такой конфигурации каждая транзакция будет ожидать подтверждение того, что на резервном сервере произошла запись транзакции в надёжное хранилище. Значение <varname>synchronous_commit</varname> может быть выставлено для отдельного пользователя, может быть прописано в файле конфигурации, для конкретного пользователя или БД или динамически изменено приложением для управления степенью надёжности на уровне отдельных транзакций.</para>

   <para>После сохранения записи о фиксации транзакции на диске ведущего сервера эта запись WAL передаётся резервному серверу. Резервный сервер отвечает подтверждающим сообщением после сохранения каждого нового блока данных WAL на диске, если только <varname>wal_receiver_status_interval</varname> на нём не равен нулю. В случае, когда выбран режим <varname>synchronous_commit</varname> <literal>remote_apply</literal>, резервный сервер передаёт подтверждение после воспроизведения записи фиксации, когда транзакция становится видимой. Если резервный сервер выбран на роль синхронного резервного, из упорядоченного списка <varname>synchronous_standby_names</varname> на ведущем, подтверждающие сообщения с этого сервера, в совокупности с сообщениями с других синхронных серверов, будут сигналом к завершению ожидания при фиксировании транзакций, требующих подтверждения сохранения записи фиксации. Эти параметры позволяют администратору определить, какие резервные серверы будут синхронными резервными. Заметьте, что настройка синхронной репликации в основном осуществляется на главном сервере. Перечисленные в списке резервных серверы должны быть подключены к нему непосредственно; он ничего не знает о резервных серверах, подключённых каскадно, через промежуточные серверы.</para>

   <para>Если <varname>synchronous_commit</varname> имеет значение <literal>remote_write</literal>, то в случае подтверждения транзакции ответ от резервного сервера об успешном подтверждении будет передан, когда данные запишутся в операционной системе, но не когда данные будет реально сохранены на диске. При таком значении уровень надёжности снижается по сравнению со значением <literal>on</literal>. Резервный сервер может потерять данные в случае падения операционной системы, но не в случае падения <productname>&productname;</productname>. Тем не менее, этот вариант полезен на практике, так как позволяет сократить время отклика для транзакции. Потеря данных может произойти только в случае одновременного сбоя ведущего и резервного, осложнённого повреждением БД на ведущем.</para>

   <para>Если <varname>synchronous_commit</varname> имеет значение <literal>remote_apply</literal>, то для завершения фиксирования транзакции потребуется дождаться, чтобы текущие синхронные резервные серверы сообщили, что они воспроизвели транзакцию и её могут видеть запросы пользователей. В простых случаях это позволяет обеспечить обычный уровень согласованности и распределение нагрузки.</para>

   <para>Пользователи прекратят ожидание в случае запроса на быструю остановку сервера. В то время как при использовании асинхронной репликации сервер не будет полностью остановлен, пока все исходящие записи WAL не переместятся на текущий присоединённый резервный сервер.</para>

   </sect3>

   <sect3 id="synchronous-replication-multiple-standbys">
    <title>Несколько синхронных резервных серверов</title>

   <para>Синхронная репликация поддерживает применение одного или нескольких синхронных резервных серверов; транзакции будут ждать, пока все резервные серверы, считающиеся синхронными, не подтвердят получение своих данных. Число синхронных резервных серверов, от которых транзакции должны ждать подтверждения, задаётся в параметре <varname>synchronous_standby_names</varname>. В этом параметре также задаётся список имён резервных серверов, определяющий и приоритет каждого сервера при выборе на роль синхронного резервного. Серверы, имена которых идут в начале списка, имеют больший приоритет и выбираются на роль синхронных. Другие резервные серверы, идущие в этом списке за ними, считаются потенциальными синхронными. Если один из текущих синхронных резервных серверов по какой-либо причине отключается, он будет немедленно заменён следующим по порядку резервным сервером.</para>
   <para>Пример значения <varname>synchronous_standby_names</varname> для нескольких синхронных резервных серверов: <programlisting>synchronous_standby_names = '2 (s1, s2, s3)'</programlisting> В данном примере, если работают четыре резервных сервера <literal>s1</literal>, <literal>s2</literal>, <literal>s3</literal> и <literal>s4</literal>, два сервера <literal>s1</literal> и <literal>s2</literal> будут выбраны на роль синхронных резервных, так как их имена идут в начале этого списка. Сервер <literal>s3</literal> будет потенциальным резервным, и возьмёт на себя роль синхронного резервного при отказе <literal>s1</literal> или <literal>s2</literal>. Сервер <literal>s4</literal> будет асинхронным резервным, так как его имя в этом списке отсутствует.</para>
   </sect3>

   <sect3 id="synchronous-replication-performance">
    <title>Планирование производительности</title>

   <para>Организуя синхронную репликацию, обычно нужно обстоятельно обдумать конфигурацию и размещение резервных серверов, чтобы обеспечить приемлемую производительность приложений. Ожидание не потребляет системные ресурсы, но блокировки транзакций будут сохраняться до подтверждения передачи. Как следствие, непродуманное использование синхронной репликации приведёт к снижению производительности БД из-за увеличения времени отклика и числа конфликтов.</para>

   <para><productname>&productname;</productname> позволяет разработчикам выбрать требуемый уровень надёжности, обеспечиваемый при репликации. Он может быть установлен для системы в целом, для отдельного пользователя или соединения или даже для отдельной транзакции.</para>

   <para>Например, в рабочей нагрузке приложения 10% изменений могут относиться к важным данным клиентов, а 90% — к менее критичным данным, потеряв которые, бизнес вполне сможет выжить (например, это могут быть текущие разговоры пользователей между собой).</para>

   <para>При настройке уровня синхронности репликации на уровне приложения (на ведущем) можно задать синхронную репликацию для большинства важных изменений без замедления общего рабочего ритма. Возможность настройки на уровне приложения является важным и практичным средством для получения выгод синхронной репликации при высоком быстродействии.</para>

   <para>Следует иметь в виду, что пропускная способность сети должна быть больше скорости генерирования данных WAL.</para>

   </sect3>

   <sect3 id="synchronous-replication-ha">
    <title>Планирование отказоустойчивости</title>

   <para>В <varname>synchronous_standby_names</varname> задаётся количество и имена синхронных резервных серверов, от которых будет ожидаться подтверждение при фиксировании транзакции, когда параметру <varname>synchronous_commit</varname> присвоено значение <literal>on</literal>, <literal>remote_apply</literal> или <literal>remote_write</literal>. Фиксирование транзакции в таком режиме может не завершиться никогда, если один из синхронных резервных серверов выйдет из строя.</para>

   <para>Поэтому для высокой степени доступности лучше всего обеспечить наличие синхронных резервных серверов в должном количестве. Для этого можно перечислить несколько потенциальных резервных серверов в строке <varname>synchronous_standby_names</varname>. Фактически синхронными резервными серверами станут серверы, имена которых стоят в этом списке первыми. Следующие за ними серверы будут становиться синхронными резервными при отказе одного из текущих.</para>

   <para>Когда к ведущему серверу впервые присоединяется резервный, он ещё не будет полностью синхронизированным. Это называется состоянием <literal>навёрстывания</literal>. Как только отставание резервного от ведущего сервера сократится до нуля в первый раз, система перейдет в состояние <literal>потоковой передачи</literal> в реальном времени. Сразу после создания резервного сервера навёрстывание может быть длительным. В случае выключения резервного сервера длительность этого процесса увеличится соответственно продолжительности простоя. Резервный сервер может стать синхронным только по достижении состояния <literal>потоковой передачи</literal>.</para>

   <para>Если ведущий сервер перезапускается при наличии зафиксированных транзакций, ожидающих подтверждения, эти транзакции будут помечены как полностью зафиксированные после восстановления ведущего. При этом нельзя гарантировать, что все резервные серверы успели получить все текущие данные WAL к моменту падения ведущего. Таким образом, некоторые транзакции могут считаться незафиксированными на резервном сервере, даже если они считаются зафиксированными на ведущем. Гарантия, которую мы можем дать, состоит в том, что приложение не получит явного подтверждения успешной фиксации, пока не будет уверенности, что данные WAL получены всеми синхронными резервными серверами.</para>

   <para>Если запустить синхронные резервные серверы в указанном количестве не удаётся, вам следует уменьшить число синхронных серверов, подтверждения которых требуются для завершения фиксации транзакций, в параметре <varname>synchronous_standby_names</varname> (или вовсе отключить его) и перезагрузить файл конфигурации на ведущем сервере.</para>

   <para>В случае если ведущий сервер стал недоступным для оставшихся резервных, следует переключиться на наиболее подходящий из имеющихся резервных серверов.</para>

   <para>Если необходимо пересоздать резервный сервер при наличии ожидающей подтверждения транзакции необходимо убедиться, что команды pg_start_backup() и pg_stop_backup() запускаются в сессии с установленным <varname>synchronous_commit</varname> = <literal>off</literal>, в противном случае эти запросы на подтверждение будут бесконечными для вновь возникшего резервного сервера.</para>

   </sect3>
  </sect2>

  <sect2 id="continuous-archiving-in-standby">
   <title>Непрерывное архивирование на резервном сервере</title>

   <indexterm><primary>непрерывное архивирование</primary> <secondary>на резервном сервере</secondary></indexterm>

   <para>Когда на резервном сервере применяется последовательное архивирование WAL, возможны два различных сценария: архив WAL может быть общим для ведущего и резервного сервера, либо резервный сервер может иметь собственный архив WAL. Когда резервный работает с собственным архивом WAL, установите в <varname>archive_mode</varname> значение <literal>always</literal>, и он будет вызывать команду архивации для каждого сегмента WAL, который он получает при восстановлении из архива или потоковой репликации. В случае с общим архивом можно поступить аналогично, но <varname>archive_command</varname> должна проверять, нет ли в архиве файла, идентичного архивируемому. Таким образом, команда <varname>archive_command</varname> должна позаботиться о том, чтобы существующий файл не был заменён файлом с другим содержимым, а в случае попытки повторного архивирования должна сообщать об успешном выполнении. При этом все эти действия должны быть рассчитаны на условия гонки, возможные, если два сервера попытаются архивировать один и тот же файл одновременно.</para>

   <para>Если в <varname>archive_mode</varname> установлено значение <literal>on</literal>, архивация в режиме восстановления или резерва не производится. В случае повышения резервного сервера, он начнёт архивацию после повышения, но в архив не попадут те файлы WAL, которые генерировал не он сам. Поэтому, чтобы в архиве оказался полный набор файлов WAL, необходимо обеспечить архивацию всех файлов WAL до того, как они попадут на резервный сервер. Это естественным образом происходит при трансляции файлов журналов, так как резервный сервер может восстановить только файлы, которые находятся в архиве, однако при потоковой репликации это не так. Когда сервер работает не в режиме резерва, различий между режимами <literal>on</literal> и <literal>always</literal> нет.</para>
  </sect2>
  </sect1>

  <sect1 id="warm-standby-failover">
   <title>Отработка отказа</title>

   <para>Если ведущий сервер отказывает, резервный должен начать процедуры отработки отказа.</para>

   <para>Если отказывает резервный сервер, никакие действия по отработке отказа не требуются. Если резервный сервер будет перезапущен, даже через некоторое время, немедленно начнётся операция восстановления, благодаря возможности возобновляемого восстановления. Если вернуть резервный сервер в строй невозможно, необходимо создать полностью новый экземпляр резервного сервера.</para>

   <para>Когда ведущий сервер отказывает и резервный сервер становится новым ведущим, а затем старый ведущий включается снова, необходим механизм для предотвращения возврата старого к роли ведущего. Иногда его называют <acronym>STONITH</acronym> (Shoot The Other Node In The Head, &laquo;Выстрелите в голову другому узлу&raquo;), что позволяет избежать ситуации, когда обе системы считают себя ведущими, и в результате возникают конфликты и потеря данных.</para>

   <para>Во многих отказоустойчивых конструкциях используются всего две системы: ведущая и резервная, с некоторым контрольным механизмом, который постоянно проверяет соединение между ними и работоспособность ведущей. Также возможно применение третьей системы (называемой следящим сервером) для исключения некоторых вариантов нежелательной отработки отказа, но эта дополнительная сложность оправдана, только если вся схема достаточно хорошо продумана и тщательно протестирована.</para>

   <para><productname>&productname;</productname> не предоставляет системного программного обеспечения, необходимого для определения сбоя на ведущем и уведомления резервного сервера баз данных. Имеется множество подобных инструментов, которые хорошо интегрируются со средствами операционной системы, требуемыми для успешной отработки отказа, например, для миграции IP-адреса.</para>

   <para>Когда происходит переключение на резервный сервер, только один сервер продолжает работу. Это состояние называется ущербным. Бывший резервный сервер теперь является ведущим, а бывший ведущий отключён и может оставаться отключённым. Для возвращения к нормальному состоянию необходимо запустить новый резервный сервер, либо на бывшем ведущем, либо в третьей, возможно, новой системе. Ускорить этот процесс в больших кластерах позволяет утилита <xref linkend="app-pgrewind"/>. По завершении этого процесса можно считать, что ведущий и резервный сервер поменялись ролями. Некоторые используют третий сервер в качестве запасного для нового ведущего, пока не будет воссоздан новый резервный сервер, хотя это, очевидно, усложняет конфигурацию системы и рабочие процедуры.</para>

   <para>Таким образом, переключение с ведущего сервера на резервный может быть быстрым, но требует некоторого времени для повторной подготовки отказоустойчивого кластера. Регулярные переключения с ведущего сервера на резервный полезны, так как при этом появляется плановое время для отключения и проведения обслуживания. Это также позволяет убедиться в работоспособности механизма отработки отказа и гарантировать, что он действительно будет работать, когда потребуется. Эти административные процедуры рекомендуется документировать письменно.</para>

   <para>Чтобы сделать ведущим резервный сервер, принимающий журналы, выполните команду <command>pg_ctl promote</command> или создайте файл-триггер с именем и путём, заданным в параметре <varname>trigger_file</varname> в файле <filename>recovery.conf</filename>. Если для переключения планируется использовать команду <command>pg_ctl promote</command>, указывать <varname>trigger_file</varname> не требуется. Если резервный сервер применяется для анализа данных, чтобы только разгрузить ведущий, выполняя запросы на чтение, а не обеспечивать отказоустойчивость, повышать его до ведущего не понадобится.</para>
  </sect1>

  <sect1 id="log-shipping-alternative">
   <title>Другие методы трансляции журнала</title>

   <para>Встроенному режиму резерва, описанному в предыдущем разделе, есть альтернатива — задать в <varname>restore_command</varname> команду, следящую за содержимым архива. Эта возможность доступна только для версии 8.4 и выше. В такой конфигурации режим <varname>standby_mode</varname> выключается, так как реализуется отдельный механизм слежения за данными, требующихся для резервного сервера. См. модуль <xref linkend="pgstandby"/> для примера реализации такой возможности.</para>

   <para>Необходимо отметить, что в этом режиме сервер будет применять только один файл WAL одновременно, то есть если использовать резервный сервер для запросов (см. сервер горячего резерва), будет задержка между операциями на главном и моментом видимости этой операции резервным, соответствующей времени заполнения файла WAL. <varname>archive_timeout</varname> можно использовать для снижения этой задержки. Так же необходимо отметить, что нельзя совмещать этот метод с потоковой репликацией.</para>

   <para>В процессе работы на ведущем сервере и резервном будет происходить обычное формирование архивов и их восстановление. Единственной точкой соприкосновения двух серверов будут только архивы файлов WAL на обеих сторонах: на ведущем архивы формируются, на резервном происходит чтение данных из архивов. Следует внимательно следить за тем, чтобы архивы WAL от разных ведущих серверов не смешивались или не перепутывались. Архив не должен быть больше, чем это необходимо для работы резерва.</para>

   <para>Магия, заставляющая работать вместе два плотно связанных сервера, проста: <varname>restore_command</varname> выполняется на резервном для запроса следующего файла WAL, ожидает его доступности на ведущем. Команда <varname>restore_command</varname> задаётся в файле <filename>recovery.conf</filename> на резервном сервере. Обычно процесс восстановления запрашивает файл из архива WAL, сообщая об ошибке в случае его недоступности. Для работы резервного сервера недоступность очередного файла WAL является обычной ситуацией, резервный просто ожидает его появления. Для файлов, оканчивающихся на <literal>.backup</literal> или <literal>.history</literal> не требуется ожидания, поэтому возвращается ненулевой код. Ожидающая <varname>restore_command</varname> может быть написана как пользовательский скрипт, который в цикле опрашивает, не появился ли очередной файл WAL. Так же должен быть способ инициировать переключение роли, при котором цикл в <varname>restore_command</varname> должен прерваться, а резервный сервер должен получить ошибку &laquo;файл не найден&raquo;. При этом восстановление завершится и резервный сервер сможет станет обычным.</para>

   <para>Псевдокод для подходящей <varname>restore_command</varname>: <programlisting>triggered = false;
while (!NextWALFileReady() &amp;&amp; !triggered)
{
    sleep(100000L);         /* ждать ~0.1 сек*/
    if (CheckForExternalTrigger())
        triggered = true;
}
if (!triggered)
        CopyWALFileForRecovery();</programlisting></para>

   <para>Рабочий пример ожидающей <varname>restore_command</varname> представлен в модуле <xref linkend="pgstandby"/>. К нему следует обратится за примером правильной реализации логики, описанной выше. Он так же может быть расширен для поддержки особых конфигураций и окружений.</para>

   <para>Метод вызова переключения является важной частью планирования и архитектуры. Один из возможных вариантов — команда <varname>restore_command</varname>. Она исполняется единожды для каждого файла WAL, но процесс, запускаемый <varname>restore_command</varname>, создаётся и завершается для каждого файла, так что это не служба и не серверный процесс, и применить сигналы и реализовать их обработчик в нём нельзя. Поэтому <varname>restore_command</varname> не подходит для отработки отказа. Можно организовать переключение по тайм-ауту, в частности, связав его с известным значением <varname>archive_timeout</varname> на ведущем. Однако это не очень надёжно, так как переключение может произойти и из-за проблем в сети или загруженности ведущего сервера. В идеале для этого следует использовать механизм уведомлений, например явно создавать файл-триггер, если это возможно.</para>

  <sect2 id="warm-standby-config">
   <title>Реализация</title>

   <para>Сокращённая процедура настройки для резервного сервера с применением альтернативного метода указана ниже. Для подробностей по каждому шагу следует обратиться к указанному разделу. <orderedlist>
     <listitem>
      <para>Разверните ведущую и резервную системы, сделав их максимально одинаковыми, включая две одинаковые копии <productname>&productname;</productname> одного выпуска.</para>
     </listitem>
     <listitem>
      <para>Настройте постоянную архивацию с ведущего сервера в каталог архивов WAL на резервном. Убедитесь, что <xref linkend="guc-archive-mode"/>, <xref linkend="guc-archive-command"/> и <xref linkend="guc-archive-timeout"/> установлены в соответствующие значения на ведущем (см. <xref remap="4" linkend="backup-archiving-wal"/>).</para>
     </listitem>
     <listitem>
      <para>Создайте базовую копию данных ведущего сервера (см. <xref remap="4" linkend="backup-base-backup"/>) и восстановите её на резервном.</para>
     </listitem>
     <listitem>
      <para>Запустите восстановление на резервном сервере из локального архива WAL с помощью команды <varname>restore_command</varname> из файла <filename>recovery.conf</filename> как описано выше (см. <xref remap="4" linkend="backup-pitr-recovery"/>).</para>
     </listitem>
    </orderedlist></para>

   <para>Поток восстановления только читает архив WAL, поэтому, как только файл WAL скопирован на резервную систему, его можно копировать на ленту в то время, как его читает резервный сервер. Таким образом, работа резервного сервера в целях отказоустойчивости может быть совмещена с долговременным сохранением файлов для восстановления после катастрофических сбоев.</para>

   <para>Для целей тестирования возможен запуск ведущего и резервного сервера в одной системе. Это не обеспечивает надёжность серверов, так же как и не подходит под описание высокой доступности.</para>
  </sect2>

  <sect2 id="warm-standby-record">
   <title>Построчная трансляция журнала</title>

   <para>Так же возможна реализация построчной трансляции журналов с применением альтернативного метода, хотя это требует дополнительных доработок, а изменения будут видны для запросов на сервере горячего резерва только после передачи полного файла WAL.</para>

   <para>Внешняя программа может вызвать функцию <function>pg_xlogfile_name_offset()</function> (см. <xref remap="4" linkend="functions-admin"/>) для поиска имени файла и точного смещения в нём от текущего конца WAL. Можно получить доступ к файлу WAL напрямую и скопировать данные из последнего известного окончания WAL до текущего окончания на резервном сервере. При таком подходе интервал возможной потери данных определяется временем цикла работы программы копирования, что может составлять очень малую величину. Так же не потребуется напрасно использовать широкую полосу пропускания для принудительного архивирования частично заполненного файла сегмента. Следует отметить, что на резервном сервере скрипт команды <varname>restore_command</varname> работает только с файлом WAL целиком, таким образом, копирование данных нарастающим итогом не может быть выполнено на резервном обычными средствами. Это используется только в случае отказа ведущего &mdash; когда последний частично сформированный файл WAL предоставляется резервному непосредственно перед переключением. Корректная реализация этого процесса требует взаимодействия скрипта команды <varname>restore_command</varname> с данными из программы копирования.</para>

   <para>Начиная с <productname>PostgreSQL</productname> версии 9.0 можно использовать потоковую репликацию (см. <xref remap="4" linkend="streaming-replication"/>) для получения этих же преимуществ меньшими усилиями.</para>
  </sect2>
 </sect1>

 <sect1 id="hot-standby">
  <title>Горячий резерв</title>

  <indexterm zone="high-availability"><primary>горячий резерв</primary></indexterm>

   <para>Термин &laquo;горячий резерв&raquo; используется для описания возможности подключаться к серверу и выполнять запросы на чтение, в то время как сервер находится в режиме резерва или восстановления архива. Это полезно и для целей репликации, и для восстановления желаемого состояния из резервной копии с высокой точностью. Так же термин «горячий резерв» описывает способность сервера переходить из режима восстановления к обычной работе, в то время как пользователи продолжают выполнять запросы и/или их соединения остаются открытыми.</para>

   <para>В режиме горячего резерва запросы выполняются примерно так же, как и в обычном режиме, с некоторыми отличиями в использовании и администрировании, описанными ниже.</para>

  <sect2 id="hot-standby-users">
   <title>Обзор на уровне пользователя</title>

   <para>Когда параметр <xref linkend="guc-hot-standby"/> на резервном сервере установлен в true, то он начинает принимать соединения сразу как только система придёт в согласованное состояние в процессе восстановления. Для таких соединений будет разрешено только чтение, запись невозможна даже во временные таблицы.</para>

   <para>Для того, чтобы данные с ведущего сервера были получены на резервном, требуется некоторое время. Таким образом, имеется измеряемая задержка между ведущим и резервным серверами. Поэтому запуск одинаковых запросов примерно в одно время на ведущем и резервном серверах может вернуть разный результат. Можно сказать, что данные на резервном сервере <firstterm>в конечном счёте согласуются</firstterm> с ведущим. После того, как запись о зафиксированной транзакции воспроизводится на резервном сервере, изменения, совершённые в этой транзакции, становится видны в любых последующих снимках данных на резервном сервере. Снимок может быть сделан в начале каждого запроса или в начале каждой транзакции в зависимости от уровня изоляции транзакции. Более подробно см. <xref remap="4" linkend="transaction-iso"/>.</para>

   <para>Транзакции, запущенные в режиме горячего резерва, могут выполнять следующие команды: <itemizedlist>
     <listitem>
      <para>Доступ к данным — <command>SELECT</command>, <command>COPY TO</command></para>
     </listitem>
     <listitem>
      <para>Команды для работы с курсором — <command>DECLARE</command>, <command>FETCH</command>, <command>CLOSE</command></para>
     </listitem>
     <listitem>
      <para>Параметры — <command>SHOW</command>, <command>SET</command>, <command>RESET</command></para>
     </listitem>
     <listitem>
      <para>Команды явного управления транзакциями <itemizedlist>
         <listitem>
          <para><command>BEGIN</command>, <command>END</command>, <command>ABORT</command>, <command>START TRANSACTION</command></para>
         </listitem>
         <listitem>
          <para><command>SAVEPOINT</command>, <command>RELEASE</command>, <command>ROLLBACK TO SAVEPOINT</command></para>
         </listitem>
         <listitem>
          <para>Блок <command>EXCEPTION</command> и другие внутренние подчиненные транзакции</para>
         </listitem>
        </itemizedlist></para>
     </listitem>
     <listitem>
      <para><command>LOCK TABLE</command>, только когда исполняется в явном виде в следующем режиме: <literal>ACCESS SHARE</literal>, <literal>ROW SHARE</literal> или <literal>ROW EXCLUSIVE</literal>.</para>
     </listitem>
     <listitem>
      <para>Планы и ресурсы — <command>PREPARE</command>, <command>EXECUTE</command>, <command>DEALLOCATE</command>, <command>DISCARD</command></para>
     </listitem>
     <listitem>
      <para>Дополнения и расширения — <command>LOAD</command></para>
     </listitem>
    </itemizedlist></para>

   <para>Транзакции, запущенные в режиме горячего резерва, никогда не получают ID транзакции и не могут быть записаны в журнал предзаписи. Поэтому при попытке выполнить следующие действия возникнут ошибки: <itemizedlist>
     <listitem>
      <para>Команды манипуляции данными (DML) — <command>INSERT</command>, <command>UPDATE</command>, <command>DELETE</command>, <command>COPY FROM</command>, <command>TRUNCATE</command>. Следует отметить, что нет разрешённых действий, которые приводили бы к срабатыванию триггера во время исполнения на резервном сервере. Это ограничение так же касается и временных таблиц, так как строки таблицы не могут быть прочитаны или записаны без обращения к ID транзакции, что в настоящее время не возможно в среде горячего резерва.</para>
     </listitem>
     <listitem>
      <para>Команды определения данных (DDL) — <command>CREATE</command>, <command>DROP</command>, <command>ALTER</command>, <command>COMMENT</command>. Эти ограничения так же относятся и к временным таблицам, так как операции могут потребовать обновления таблиц системных каталогов.</para>
     </listitem>
     <listitem>
      <para><command>SELECT ... FOR SHARE | UPDATE</command>, так как блокировка строки не может быть проведена без обновления соответствующих файлов данных.</para>
     </listitem>
     <listitem>
      <para>Правила для выражений <command>SELECT</command>, которые приводят к выполнению команд DML.</para>
     </listitem>
     <listitem>
      <para><command>LOCK</command> которая явно требует режим более строгий чем <literal>ROW EXCLUSIVE MODE</literal>.</para>
     </listitem>
     <listitem>
      <para><command>LOCK</command> в короткой форме с умолчаниями, так как требует <literal>ACCESS EXCLUSIVE MODE</literal>.</para>
     </listitem>
     <listitem>
      <para>Команды управления транзакциями, которые в явном виде требуют режим не только для чтения <itemizedlist>
         <listitem>
          <para><command>BEGIN READ WRITE</command>, <command>START TRANSACTION READ WRITE</command></para>
         </listitem>
         <listitem>
          <para><command>SET TRANSACTION READ WRITE</command>, <command>SET SESSION CHARACTERISTICS AS TRANSACTION READ WRITE</command></para>
         </listitem>
         <listitem>
          <para>
           <command>SET transaction_read_only = off</command>
          </para>
         </listitem>
        </itemizedlist></para>
     </listitem>
     <listitem>
      <para>Команды двухфазной фиксации — <command>PREPARE TRANSACTION</command>, <command>COMMIT PREPARED</command>, <command>ROLLBACK PREPARED</command>, так как даже транзакции только для чтения нуждаются в записи в WAL на подготовительной фазе (первая фаза двухфазной фиксации).</para>
     </listitem>
     <listitem>
      <para>Обновление последовательностей — <function>nextval()</function>, <function>setval()</function></para>
     </listitem>
     <listitem>
      <para><command>LISTEN</command>, <command>UNLISTEN</command>, <command>NOTIFY</command></para>
     </listitem>
    </itemizedlist></para>

   <para>При обычной работе транзакции <quote>только для чтения</quote> могут использовать команды <command>LISTEN</command>, <command>UNLISTEN</command> и <command>NOTIFY</command>; таким образом, сеансы горячего резерва работают с несколько большими ограничениями, чем обычные только читающие сеансы. Возможно, что некоторые из этих ограничений будут ослаблены в следующих выпуска.</para>

   <para>В режиме горячего резерва параметр <varname>transaction_read_only</varname> всегда имеет значение true и изменить его нельзя. Но если не пытаться модифицировать содержимое БД, подключение к серверу в этом режиме не отличается от подключений к обычным базам данных. При отработке отказа или переключении ролей база данных переходит в обычный режим работы. Когда сервер меняет режим работы, установленные сеансы остаются подключёнными. После выхода из режима горячего резерва становится возможным запускать пишущие транзакции (даже в сеансах, начатых ещё в режиме горячего резерва).</para>

   <para>Пользователи могут узнать о нахождении сессии в режиме только для чтения с помощью команды <command>SHOW transaction_read_only</command>. Кроме того, набор функций (<xref linkend="functions-recovery-info-table"/>) позволяет пользователям получить доступ к информации о резервном сервере. Это позволяет создавать программы, учитывающие текущий статус базы данных. Такой режим может быть полезен для мониторинга процесса восстановления или для написания комплексного восстановления для особенных случаев.</para>
  </sect2>

  <sect2 id="hot-standby-conflict">
   <title>Обработка конфликтов запросов</title>

   <para>Ведущий и резервный серверы связаны между собой многими слабыми связями. События на ведущем сервере оказывают влияние на резервный. В результате имеется потенциальная возможность отрицательного влияния или конфликта между ними. Наиболее простой для понимания конфликт — быстродействие: если на ведущем происходит загрузка очень большого объёма данных, то происходит создание соответствующего потока записей WAL на резервный сервер. Таким образом, запросы на резервном конкурируют за системные ресурсы, например, ввод-вывод.</para>

   <para>Так же может возникнуть дополнительный тип конфликта на сервере горячего резерва. Этот конфликт называется <emphasis>жёстким конфликтом</emphasis>, оказывает влияние на запросы, приводя к их отмене, а в некоторых случаях и к обрыву сессии для разрешения конфликтов. Пользователям предоставлен набор средств для обработки подобных конфликтов. Случаи конфликтов включают: <itemizedlist>
       <listitem>
        <para>Установка эксклюзивной блокировки на ведущем сервере, как с помощью явной команды <command>LOCK</command>, так и при различных <acronym>DDL</acronym>, что приводит к конфликту доступа к таблицам на резервном.</para>
       </listitem>
       <listitem>
        <para>Удаление табличного пространства на ведущем сервере приводит к конфликту на резервном когда запросы используют это пространство для хранения временных рабочих файлов.</para>
       </listitem>
       <listitem>
        <para>Удаление базы данных на ведущем сервере конфликтует с сессиями, подключёнными к этой БД на резервном.</para>
       </listitem>
       <listitem>
        <para>Приложение очистки устаревших транзакций из WAL конфликтует с транзакциями на резервном сервере, которые используют снимок данных, который всё ещё видит какие-то из очищенных на ведущем строк.</para>
       </listitem>
       <listitem>
        <para>Приложение очистки устаревших транзакций из WAL конфликтует с запросами к целевой странице на резервном сервере вне зависимости от того, являются ли данные удалёнными или видимыми.</para>
       </listitem>
      </itemizedlist></para>

   <para>В этих случаях на ведущем сервере просто происходит ожидание; пользователю следует выбрать какую их конфликтующих сторон отменить. Тем не менее, на резервном нет выбора: действия из WAL уже произошли на ведущем, поэтому резервный обязан применить их. Более того, позволять обработчику WAL ожидать неограниченно долго может быть крайне нежелательно, так как отставание резервного сервера от ведущего может всё возрастать. Таким образом, механизм обеспечивает принудительную отмену запросов на резервном сервере, которые конфликтуют с применяемыми записями WAL.</para>

   <para>Примером такой проблемы может быть ситуация: администратор на ведущем сервере выполнил команду <command>DROP TABLE</command> для таблицы, которая сейчас участвует в запросе на резервном. Понятно, что этот запрос нельзя будет выполнять дальше, если команда <command>DROP TABLE</command> применится на резервном. Если бы этот запрос выполнялся на ведущем, команда <command>DROP TABLE</command> ждала бы его окончания. Но когда на ведущем выполняется только команда <command>DROP TABLE</command>, ведущий сервер не знает, какие запросы выполняются на резервном, поэтому он не может ждать завершения подобных запросов. Поэтому если записи WAL с изменением прибудут на резервный сервер, когда запрос будет продолжать выполняться, возникнет конфликт. В этом случае резервный сервер должен либо задержать применение этих записей WAL (и всех остальных, следующих за ними), либо отменить конфликтующий запрос, чтобы можно было применить <command>DROP TABLE</command>.</para>

   <para>Если конфликтный запрос короткий, обычно желательно разрешить ему завершиться, ненадолго задержав применение записей WAL, но слишком большая задержка в применении WAL обычно нежелательна. Поэтому механизм отмены имеет параметры <xref linkend="guc-max-standby-archive-delay"/> и <xref linkend="guc-max-standby-streaming-delay"/>, которые определяют максимально допустимое время задержки применения WAL. Конфликтующие запросы будут отменены, если они длятся дольше допустимого времени задержки применения очередных записей WAL. Два параметра существуют для того, чтобы можно было задать разные значения для чтения записей WAL из архива (то есть при начальном восстановлении из базовой копии либо при <quote>навёрстывании</quote> ведущего сервера в случае большого отставания) и для получения записей WAL при потоковой репликации.</para>

   <para>На резервном сервере, созданном преимущественно для отказоустойчивости, лучше выставлять параметры задержек относительно небольшими, чтобы он не мог сильно отстать от ведущего из-за задержек, связанных с ожиданием запросов горячего резерва. Однако если резервный сервер предназначен для выполнения длительных запросов, то высокое значение или даже бесконечное ожидание могут быть предпочтительнее. Тем не менее, следует иметь в виду, что длительные запросы могут оказать влияние на другие сессии на резервном сервере в виде отсутствия последних изменений от ведущего из-за задержки применения записей WAL.</para>

   <para>В случае, если задержка, определённая <varname>max_standby_archive_delay</varname> или <varname>max_standby_streaming_delay</varname> будет превышена, конфликтующий запрос будет отменён. Обычно это выражается в виде ошибки отмены, но в случае проигрывания команды <command>DROP DATABASE</command> обрывается вся конфликтная сессия. Так же, если конфликт произошел при блокировке, вызванной транзакцией в состоянии IDLE, конфликтная сессия разрывается (это поведение может изменить в будущем).</para>

   <para>Отменённые запросы могут быть немедленно повторены (конечно после старта новой транзакции). Так как причина отмены зависит от природы проигрываемых записей WAL, запрос, который был отменён, может быть успешно выполнен вновь.</para>

   <para>Следует учесть, что параметры задержки отсчитываются от времени получения резервным сервером данных WAL. Таким образом, период дозволенной работы для запроса на резервном сервере никогда не может быть длиннее параметра задержки и может быть существенно короче, если резервный уже находится в режиме задержки в результате ожидания предыдущего запроса или результат не доступен из-за высокой нагрузки обновлений.</para>

   <para>Наиболее частой причиной конфликтов между запросами на резервном сервере и проигрыванием WAL является преждевременная очистка. Обычно <productname>&productname;</productname> допускает очистку старых версий записей при условии что ни одна из транзакций их не видит согласно правилам видимости данных для MVCC. Тем не менее, эти правила применяются только для транзакций, выполняемых на главном сервере. Таким образом, допустима ситуация, когда на главном запись уже очищена, но эта же запись всё ещё видна для транзакций на резервном сервере.</para>

   <para>Для опытных пользователей следует отметить, что как очистка старых версий строк, так и заморозка версии строки могут потенциально вызвать конфликт с запросами на резервном сервере. Ручной запуск команды <command>VACUUM FREEZE</command> может привести к конфликту, даже в таблице без обновленных и удалённых строк.</para>

   <para>Пользователи должны понимать, что регулярное и активное изменение данных в таблицах на ведущем сервере чревато отменой длительных запросов на резервном. В таком случае установка конечного значения для <varname>max_standby_archive_delay</varname> или <varname>max_standby_streaming_delay</varname> действует подобно ограничению <varname>statement_timeout</varname>.</para>

   <para>В случае, если количество отменённых запросов на резервном сервере получается неприемлемым, существует ряд дополнительных возможностей. Первая возможность — установить параметр <varname>hot_standby_feedback</varname>, который не даёт команде <command>VACUUM</command> удалять записи, ставшие недействительными недавно, что предотвращает конфликты очистки. При этом следует учесть, что это вызывает задержку очистки мёртвых строк на ведущем, что может привести к нежелательному распуханию таблицы. Тем не менее, в итоге ситуация будет не хуже, чем если бы запросы к резервному серверу исполнялись непосредственно на ведущем, но при этом сохранится положительный эффект от разделения нагрузки. В случае, когда соединение резервных серверов с ведущим часто разрывается, следует скорректировать период, в течение которого обратная связь через <varname>hot_standby_feedback</varname> не обеспечивается. Например, следует подумать об увеличении <varname>max_standby_archive_delay</varname>, чтобы запросы отменялись не сразу при конфликтах с архивом WAL в период разъединения. Также может иметь смысл увеличить <varname>max_standby_streaming_delay</varname> для предотвращения быстрой отмены запросов из-за полученных записей WAL после восстановления соединения.</para>

   <para>Другая возможность — увеличение <xref linkend="guc-vacuum-defer-cleanup-age"/> на ведущем сервере таким образом, чтобы мёртвые записи не очищались бы так быстро, как при обычном режиме работы. Это даёт запросам на резервном сервере больше времени на выполнение, прежде чем они могут быть отменены, без увеличения задержки <varname>max_standby_streaming_delay</varname>. Тем не менее при таком подходе очень трудно обеспечить какое-то определённое окно по времени, так как <varname>vacuum_defer_cleanup_age</varname> измеряется в количестве транзакций, выполняемых на ведущем сервере.</para>

   <para>Количество отменённых запросов и причины отмены можно просмотреть через системное представление <structname>pg_stat_database_conflicts</structname> на резервном сервере. Системное представление <structname>pg_stat_database</structname> так же содержит итоговую информацию.</para>
  </sect2>

  <sect2 id="hot-standby-admin">
   <title>Обзор административной части</title>

   <para>Если в файле <filename>postgresql.conf</filename> для параметра <varname>hot_standby</varname> задано значение <literal>on</literal> и существует файл <filename>recovery.conf</filename>, сервер запустится в режиме горячего резерва. Однако может пройти некоторое время, прежде чем к нему можно будет подключиться, так как он не будет принимать подключения, пока не произведёт восстановление до согласованного состояния, подходящего для выполнения запросов. (Информация о согласованности состояния записывается на ведущем сервере в контрольной точке.) В течение этого периода клиенты при попытке подключения будут получать сообщение об ошибке. Убедиться, что сервер включился в работу, можно либо повторяя попытки подключения из приложения до успешного подключения, либо дождавшись появления в журналах сервера этих сообщений: <programlisting>LOG:  entering standby mode

... then some time later ...

LOG:  consistent recovery state reached
LOG:  database system is ready to accept read only connections</programlisting> Включить горячий резерв нельзя, если WAL был записан в период, когда на ведущем сервере параметр <varname>wal_level</varname> имел значение не <literal>replica</literal> и не <literal>logical</literal>. Достижение согласованного состояния также может быть отсрочено, если имеют место оба этих условия: <itemizedlist>
       <listitem>
        <para>Пишущая транзакция имеет более 64 подтранзакций</para>
       </listitem>
       <listitem>
        <para>Очень длительные пишущие транзакции</para>
       </listitem>
      </itemizedlist> Если вы применяете файловую репликацию журналов (&laquo;тёплый резерв&raquo;), возможно, придётся ожидать прибытия следующего файла WAL (максимальное время ожидания задаётся параметром <varname>archive_timeout</varname> на ведущем сервере).</para>

   <para>Значения некоторых параметров на резервном сервере необходимо изменить при модификации их на ведущем. Для таких параметров значения на резервном сервере должны быть равны или больше значений на ведущем. Если параметр имеет недостаточно большое значение, резервный сервер не сможет начать работу. Следует увеличить значение и повторить попытку восстановления ещё раз. Это касается следующих параметров: <itemizedlist>
       <listitem>
        <para>
         <varname>max_connections</varname>
        </para>
       </listitem>
       <listitem>
        <para>
         <varname>max_prepared_transactions</varname>
        </para>
       </listitem>
       <listitem>
        <para>
         <varname>max_locks_per_transaction</varname>
        </para>
       </listitem>
       <listitem>
        <para>
         <varname>max_worker_processes</varname>
        </para>
       </listitem>
      </itemizedlist></para>

   <para>Очень важно для администратора выбрать подходящие значения для <xref linkend="guc-max-standby-archive-delay"/> и <xref linkend="guc-max-standby-streaming-delay"/>. Оптимальное значение зависит от приоритетов. Например, если основное назначение сервера — обеспечение высокой степени доступности, то следует установить короткий период, возможно даже нулевой, хотя это очень жёсткий вариант. Если резервный сервер планируется как дополнительный сервер для аналитических запросов, то приемлемой будет максимальная задержка в несколько часов или даже -1, что означает бесконечное ожидание окончания запроса.</para>

   <para>Вспомогательные биты статуса транзакций, записанные на ведущем, не попадают в WAL, так что они, скорее всего, будут перезаписаны на нём при работе с данными. Таким образом, резервный сервер будет производить запись на диск, даже если все пользователи только читают данные, ничего не меняя. Кроме того, пользователи будут записывать временные файлы при сортировке больших объёмов и обновлять файлы кеша. Поэтому в режиме горячего резерва ни одна часть базы данных фактически не работает в режиме &laquo;только чтение&raquo;. Следует отметить, что также возможно выполнить запись в удалённую базу данных с помощью модуля <application>dblink</application> и другие операции вне базы данных с применением PL-функций, несмотря на то, что транзакции по-прежнему смогут только читать данные.</para>

   <para>Следующие типы административных команд недоступны в течение режима восстановления: <itemizedlist>
       <listitem>
        <para>Команды определения данных (DDL) — например: <command>CREATE INDEX</command></para>
       </listitem>
       <listitem>
        <para>Команды выдачи привилегий и назначения владельца — <command>GRANT</command>, <command>REVOKE</command>, <command>REASSIGN</command></para>
       </listitem>
       <listitem>
        <para>Команды обслуживания — <command>ANALYZE</command>, <command>VACUUM</command>, <command>CLUSTER</command>, <command>REINDEX</command></para>
       </listitem>
      </itemizedlist></para>

   <para>Ещё раз следует отметить, что некоторые из этих команд фактически доступны на ведущем сервере для транзакций в режиме только для чтения.</para>

   <para>В результате нельзя создать дополнительные индексы или статистику, чтобы они существовали только на резервном. Если подобные административные команды нужны, то их следует выполнить на ведущем сервере, затем эти изменения будут распространены на резервные серверы.</para>

   <para>Функции <function>pg_cancel_backend()</function> и <function>pg_terminate_backend()</function> работают на стороне пользователя, но не для процесса запуска, который обеспечивает восстановление. Представление <structname>pg_stat_activity</structname> не показывает ни вхождение для процесса запуска, ни восстановление транзакций в активном состоянии. В результате <structname>pg_prepared_xacts</structname> всегда пусто в ходе восстановления. Если требуется разрешить сомнительные подготовленные транзакции, следует обратиться к <literal>pg_prepared_xacts</literal> на ведущем и выполнить команду для разрешения транзакции там.</para>

   <para><structname>pg_locks</structname> отображает блокировки, происходящие в процессе работы сервера как обычно. <structname>pg_locks</structname> так же показывает виртуальные транзакции, обработанные процессом запуска, которому принадлежат все <literal>AccessExclusiveLocks</literal>, наложенные транзакциями в режиме восстановления. Следует отметить, что процесс запуска не запрашивает блокировки, чтобы внести изменения в базу данных, поэтому блокировки, отличные от <literal>AccessExclusiveLocks</literal> не показываются в <structname>pg_locks</structname> для процесса запуска, подразумевается их существование.</para>

   <para>Модуль <productname>check_pgsql</productname> для <productname>Nagios</productname> будет работать, так как сервер выдаёт простую информацию, наличие которой он проверяет. Скрипт мониторинга <productname>check_postgres</productname> так же работает, хотя для некоторых выдаваемых показателей результаты могут различаться или вводить в заблуждение. Например, нельзя отследить время последней очистки, так как очистка не производится на резервном сервере. Очистка запускается на ведущем сервере и результаты её работы передаются резервному.</para>

   <para>Команды контроля файлов WAL так же не будут работать во время восстановления, например: <function>pg_start_backup</function>, <function>pg_switch_xlog</function> и т. п.</para>

   <para>Динамически загружаемые модули работать будут, включая <structname>pg_stat_statements</structname>.</para>

   <para>Рекомендательная блокировка работает обычно при восстановлении, включая обнаружение взаимных блокировок. Следует отметить, что рекомендательная блокировка никогда не попадает в WAL, таким образом для рекомендательной блокировки как на ведущем сервере, так и на резервном, невозможен конфликт с проигрыванием WAL. Но возможно получение рекомендательной блокировки на ведущем сервере, а затем получение подобной рекомендательной блокировки на резервном. Рекомендательная блокировка относится только к серверу, на котором она получена.</para>

   <para>Системы репликации на базе триггеров, подобные <productname>Slony</productname>, <productname>Londiste</productname> и <productname>Bucardo</productname> не могут запускаться на резервном сервере вовсе, хотя они превосходно работают на ведущем до тех пор, пока не будет подана команда не пересылать изменения на резервный. Проигрывание WAL не основано на триггерах, поэтому поток WAL нельзя транслировать с резервного сервера в другую систему, которая требует дополнительной записи в БД или работает на основе триггеров.</para>

   <para>Новые OID не могут быть выданы, хотя, например генераторы <acronym>UUID</acronym> смогут работать, если они не пытаются записывать новое состояние в базу данных.</para>

   <para>В настоящий момент создание временных таблиц недопустимо при транзакции только для чтения, в некоторых случаях существующий скрипт будет работать неверно. Это ограничение может быть ослаблено в следующих выпусках. Это одновременно требование SQL стандарта и техническое требование.</para>

   <para>Команда <command>DROP TABLESPACE</command> может быть выполнена только если табличное пространство пусто. Некоторые пользователи резервного сервера могут активно использовать табличное пространство через параметр <varname>temp_tablespaces</varname>. Если имеются временные файлы в табличных пространствах, все активные запросы отменяются для обеспечения удаления временных файлов, затем табличное пространство может быть удалено и продолжено проигрывание WAL.</para>

   <para>Выполнение команды <command>DROP DATABASE</command> или <command>ALTER DATABASE ... SET TABLESPACE</command> на ведущем сервере приводит к созданию записи в WAL, которая вызывает принудительное отключение всех пользователей, подключённых к этой базе данных на резервном. Это происходит немедленно, вне зависимости от значения <varname>max_standby_streaming_delay</varname>. Следует отметить, что команда <command>ALTER DATABASE ... RENAME</command> не приводит к отключению пользователей, так что обычно она действует незаметно, хотя в некоторых случаях возможны сбои программ, которые зависят от имени базы данных.</para>

   <para>Если вы в обычном режиме (не в режиме восстановления) выполните <command>DROP USER</command> или <command>DROP ROLE</command> для роли с возможностью подключения, в момент, когда этот пользователь подключён, на данном пользователе это никак не отразится — он останется подключённым. Однако переподключиться он уже не сможет. Это же поведение действует в режиме восстановления — если выполнить <command>DROP USER</command> на ведущем сервере, пользователь не будет отключён от резервного.</para>

   <para>Сборщик статистики работает во время восстановления. Все операции сканирования, чтения, блоки, использование индексов и т. п. будут записаны обычным образом на резервном сервере. Действия, происходящие при проигрывании, не будут дублировать действия на ведущем сервере, то есть проигрывание команды вставки не увеличит значение столбца Inserts в представлении pg_stat_user_tables. Файлы статистики удаляются с началом восстановления, таким образом, статистика на ведущем сервере и резервном будет разной. Это является особенностью, не ошибкой.</para>

   <para>Автоматическая очистка не работает во время восстановления. Она запустится в обычном режиме после завершения восстановления.</para>

   <para>Во время восстановления активен фоновый процесс записи, он обрабатывает точки перезапуска (подобно контрольным точкам на ведущем сервере) и выполняет обычную очистку блоков. В том числе он может обновлять вспомогательные биты, сохранённые на резервном. Во время восстановления принимается команда <command>CHECKPOINT</command>, но она производит точку перезапуска, а не создаёт новую точку восстановления.</para>
  </sect2>

  <sect2 id="hot-standby-parameters">
   <title>Ссылки на параметры горячего резерва</title>

   <para>Различные параметры были упомянуты выше в <xref remap="6" linkend="hot-standby-conflict"/> и <xref remap="6" linkend="hot-standby-admin"/>.</para>

   <para>На ведущем могут применяться параметры <xref linkend="guc-wal-level"/> и <xref linkend="guc-vacuum-defer-cleanup-age"/>. Параметры <xref linkend="guc-max-standby-archive-delay"/> и <xref linkend="guc-max-standby-streaming-delay"/> на ведущем не действуют.</para>

   <para>На резервном сервере могут применяться параметры <xref linkend="guc-hot-standby"/>, <xref linkend="guc-max-standby-archive-delay"/> и <xref linkend="guc-max-standby-streaming-delay"/>. Параметр <xref linkend="guc-vacuum-defer-cleanup-age"/> на нём не действует, пока сервер остаётся в режиме резервного сервера. Но если он станет ведущим, его значение вступит в силу.</para>
  </sect2>

  <sect2 id="hot-standby-caveats">
   <title>Ограничения</title>

   <para>Имеются следующие ограничения горячего резерва. Они могут и скорее всего будут исправлены в следующих выпусках: <itemizedlist>
   <listitem>
    <para>Операции с хеш-индексами в настоящее время не проходят через журнал WAL, таким образом, при воспроизведении WAL эти индексы не обновляются.</para>
   </listitem>
   <listitem>
    <para>Требуется информация о всех запущенных транзакциях перед тем как будет создан снимок данных. Транзакции, использующие большое количество подтранзакций (в настоящий момент больше 64), будут задерживать начало соединения только для чтения до завершения самой длинной пишущей транзакции. При возникновении этой ситуации поясняющее сообщение будет записано в журнал сервера.</para>
   </listitem>
   <listitem>
    <para>Подходящие стартовые точки для запросов на резервном сервере создаются при каждой контрольной точке на главном. Если резервный сервер отключается, в то время как главный был в отключённом состоянии, может оказаться невозможным возобновить его работу в режиме горячего резерва, до того, как запустится ведущий и добавит следующие стартовые точки в журналы WAL. Подобная ситуация не является проблемой для большинства случаев, в которых она может произойти. Обычно, если ведущий сервер выключен и больше не доступен, это является следствием серьёзного сбоя и в любом случае требует преобразования резервного в новый ведущий. Так же в ситуации, когда ведущий отключён намеренно, проверка готовности резервного к преобразованию в ведущий тоже является обычной процедурой.</para>
   </listitem>
   <listitem>
    <para>В конце восстановления блокировки <literal>AccessExclusiveLocks</literal>, вызванные подготовленными транзакциями, требуют удвоенное, в сравнении с нормальным, количество блокировок записей таблицы. Если планируется использовать либо большое количество конкурирующих подготовленных транзакций, обычно вызывающие <literal>AccessExclusiveLocks</literal>, либо большие транзакции с применением большого количества <literal>AccessExclusiveLocks</literal>, то рекомендуется выбрать большое значение параметра <varname>max_locks_per_transaction</varname>, возможно в два раза большее, чем значение параметра на ведущем сервере. Всё это не имеет значения, когда <varname>max_prepared_transactions</varname> равно 0.</para>
   </listitem>
   <listitem>
    <para>Уровень изоляции транзакции Serializable в настоящее время недоступен в горячем резерве. (За подробностями обратитесь к <xref remap="3" linkend="xact-serializable"/> и <xref remap="3" linkend="serializable-consistency"/>) Попытка выставить для транзакции такой уровень изоляции в режиме горячего резерва вызовет ошибку.</para>
   </listitem>
  </itemizedlist></para>
  </sect2>

 </sect1>

</chapter>
