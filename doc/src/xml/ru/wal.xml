<!-- doc/src/xml/wal.xml -->

<chapter id="wal">
 <title>Надёжность и журнал упреждающей записи</title>

 <para>В данной главе рассказывается, как для обеспечения эффективной и надёжной работы используется журнал упреждающей записи.</para>

 <sect1 id="wal-reliability">
  <title>Надёжность</title>

  <para>Надёжность — это важное свойство любой серьёзной СУБД и <productname>&productname;</productname> делает всё возможное, чтобы гарантировать надёжность своего функционирования. Один из аспектов надёжности состоит в том, что все данные записываются с помощью подтверждённых транзакций, которые сохраняются в энергонезависимой области, которая защищена от потери питания, сбоев операционной системы и аппаратных отказов (разумеется, за исключением отказа самой энергонезависимой области). Успешная запись данных в постоянное место хранения (диск или эквивалентный носитель) обычно всё, что требуется. Фактически, даже если компьютер полностью вышел из строя, если диски выжили, то они могут быть переставлены в другой похожий компьютер и все подтверждённые транзакции останутся неповреждёнными.</para>

  <para>Хотя периодическая запись данных на пластины диска может выглядеть как простая операция, это не так, потому что диски значительно медленнее, чем оперативная память и процессор, а также потому что между оперативной памятью и пластинами диска есть некоторые механизмы кеширования. Во-первых, есть буферный кеш операционной системы, который кеширует частые запросы к блокам диска и комбинирует запись на диск. К счастью, все операционные системы предоставляют приложениям способ принудительной записи из буферного кеша на диск и <productname>&productname;</productname> использует эту возможность. (Смотрите параметр <xref linkend="guc-wal-sync-method"/> который отвечает за то как это делается.)</para>

  <para>Далее, кеширование также может осуществляться контроллером диска; в особенности это касается <acronym>RAID</acronym>-контроллеров. В некоторых случаях это кеширование работает в режиме <firstterm>сквозной записи</firstterm>, что означает, что запись осуществляется на диск как только приходят данные. В других случаях, возможна работа в режиме <firstterm>отложенной записи</firstterm>, что означает, что запись осуществляется некоторое время спустя. Такой режим кеширования может создавать риск для надёжности, потому что память контроллера диска непостоянна и будет потеряна в случае потери питания. Лучшие контроллеры имеют так называемую <firstterm>батарею резервного питания</firstterm> (Battery-Backup Unit, <acronym>BBU</acronym>), которая сохраняет кеш контроллера на батарее, если пропадёт системное питание. После возобновления питания, данные, оставшиеся в кеше контроллера, будут записаны на диски.</para>

  <para>И наконец, многие диски имеют кеширование. На каких-то дисках оно работает в режиме сквозной записи, на других в режиме отложенной записи, что приводит к тем же проблемам потери данных для кеша отложенной записи, что и с кешем в контроллерах дисков. Диски IDE и SATA, потребительского класса особенно, часто имеют кеш отложенной записи, который сбрасывается при потере питания. Многие SSD-накопители также имеют зависимый от питания кеш отложенной записи.</para>

  <para>Обычно, такое кеширование можно выключить; однако, то, как это делается, различается для операционной системы и для типа диска:</para>

  <itemizedlist>
    <listitem>
      <para>В <productname>Linux</productname> параметры дисков IDE и SATA могут быть получены с помощью команды <command>hdparm -I</command>; кеширование записи включено, если за строкой <literal>Write cache</literal> следует <literal>*</literal>. Для выключения кеширования записи может быть использована команда <command>hdparm -W 0</command>. Параметры SCSI-дисков могут быть получены с помощью утилиты <ulink url="http://sg.danny.cz/sg/sdparm.html"><application>sdparm</application></ulink>. Используйте <command>sdparm --get=WCE</command>, чтобы проверить, включено ли кеширование записи, и <command>sdparm --clear=WCE</command>, чтобы выключить его.</para>
    </listitem>

    <listitem>
      <para>Во <productname>FreeBSD</productname> параметры IDE-дисков могут быть получены с помощью команды <command>atacontrol</command>, а кеширование записи выключается при помощи установки параметра <literal>hw.ata.wc=0</literal> в файле <filename>/boot/loader.conf</filename>; Для SCSI-дисков параметры могут быть получены, используя команду <command>camcontrol identify</command>, а кеширование записи изменяется при помощи утилиты <command>sdparm</command>.</para>
    </listitem>

    <listitem>
      <para>В <productname>Solaris</productname> кешированием записи на диск управляет команда <command>format -e</command>. (Использование файловой системы Solaris <acronym>ZFS</acronym>, при включённом кешировании записи на диск, является безопасным, потому что она использует собственные команды сброса кеша на диск.)</para>
    </listitem>

    <listitem>
      <para>В <productname>Windows</productname>, если параметр <varname>wal_sync_method</varname> установлен в <literal>open_datasync</literal> (по умолчанию), кеширование записи на диск может быть выключено снятием галочки <literal>My Computer\Open\<replaceable>disk drive</replaceable>\Properties\Hardware\Properties\Policies\Enable write caching on the disk</literal>. В качестве альтернативы, можно установить параметр <varname>wal_sync_method</varname> в значение <literal>fsync</literal> или <literal>fsync_writethrough</literal>, что предотвращает кеширование записи.</para>
    </listitem>

    <listitem>
      <para>В <productname>OS X</productname> кеширование записи можно отключить, установив для параметра <varname>wal_sync_method</varname> значение <literal>fsync_writethrough</literal>.</para>
    </listitem>
  </itemizedlist>

  <para>Новые модели SATA-дисков (которые соответствуют стандарту <acronym>ATAPI-6</acronym> или более позднему) предлагают команду сброса кеша на диск (<command>FLUSH CACHE EXT</command>), а SCSI-диски уже давно поддерживают похожую команду <command>SYNCHRONIZE CACHE</command>. Эти команды недоступны из <productname>&productname;</productname> напрямую, но некоторые файловые системы (например, <acronym>ZFS</acronym>, <acronym>ext4</acronym>), могут использовать их для сброса данных из кеша на пластины диска при включённом режиме кеша сквозной записи. К сожалению, такие файловые системы ведут себя неоптимально при комбинировании с батареей резервного питания (<acronym>BBU</acronym>) дискового контроллера. В таких случаях, команда синхронизации принуждает сохранять все данные на диск из кеша контроллера, сводя преимущество BBU к нулю. Вы можете запустить модуль <xref linkend="pgtestfsync"/>, чтобы увидеть, что вы попали в эту ситуацию. Если это так, преимущества производительности BBU могут быть восстановлены с помощью выключения барьеров записи для файловой системы или переконфигурирования контроллера диска, если это возможно. Если барьеры записи выключены, убедитесь, что батарея годная; при отказе батареи может произойти потеря данных. Есть надежда, что разработчики файловых систем и контроллеров дисков, в конце концов, устранят это неоптимальное поведение.</para>

  <para>Когда операционная система отправляет запрос на запись к аппаратному обеспечению для хранения данных, она мало что может сделать, чтобы убедиться, что данные действительно сохранены в какой-либо энергонезависимой области. Скорее, это является зоной ответственности администратора, убедиться в целостности данных на всех компонентах хранения. Избегайте дисковых контроллеров, которые не имеют батарей резервного питания для кеширования записи. На уровне диска, запретите режим отложенной записи, если диск не может гарантировать, что данные будут записаны перед выключением. Если вы используете SSD, знайте, что многие их них по умолчанию не выполняют команды сброса кеша на диск. Вы можете протестировать надёжность поведения подсистемы ввода/вывода, используя <ulink url="http://brad.livejournal.com/2116715.html"><filename>diskchecker.pl</filename></ulink>.</para>

  <para>Другой риск потери данных состоит в самой записи на пластины диска. Пластины диска разделяются на секторы, обычно по 512 байт каждый. Каждая операция физического чтения или записи обрабатывает целый сектор. Когда дисковый накопитель получает запрос на запись, он может соответствовать нескольким секторам по 512 байт (<productname>&productname;</productname> обычно за один раз записывает 8192 байта или 16 секторов) и из-за отказа питания процесс записи может закончится неудачей в любое время, что означает, что некоторые из 512-байтовых секторов будут записаны, а некоторые нет. Чтобы защититься от таких сбоев, <emphasis>перед</emphasis> изменением фактической страницы на диске, <productname>&productname;</productname> периодически записывает полные образы страниц на постоянное устройство хранения WAL. С помощью этого, во время восстановления после краха, <productname>&productname;</productname> может восстановить из WAL страницы, которые записаны частично. Если у вас файловая система, которая защищена от частичной записи страниц (например, ZFS), вы можете выключить работу с образами страниц, выключив параметр <xref linkend="guc-full-page-writes"/>. Батарея резервного питания (BBU) контроллера диска не защищает от частичной записи страниц, если не гарантируется, что данные записаны в BBU как полные (8kB) страницы.</para>
  <para><productname>&productname;</productname> также защищает от некоторых видов повреждения данных на устройствах хранения, которые могут произойти из-за аппаратных ошибок или из-за дефектов поверхности с течением времени, например, при операциях чтения/записи во время сборки мусора. <itemizedlist>
    <listitem>
     <para>Каждая индивидуальная запись в WAL защищена с помощью контрольной суммы по алгоритму CRC-32 (32-bit), что позволяет судить о корректности данных в записи. Значение CRC устанавливается, когда мы пишем каждую запись WAL и проверяется в ходе восстановления после сбоя, восстановления из архива, и при репликации.</para>
    </listitem>
    <listitem>
     <para>Страницы данных в настоящее время не защищаются контрольными суммами по умолчанию, хотя полные образы страниц, записанные в WAL будут защищены; смотрите <link linkend="app-initdb-data-checksums"><application>initdb</application></link> для деталей о включении в страницы данных информации о контрольных суммах.</para>
    </listitem>
    <listitem>
     <para>Для внутренних структур данных, таких как <filename>pg_clog</filename>, <filename>pg_subtrans</filename>, <filename>pg_multixact</filename>, <filename>pg_serial</filename>, <filename>pg_notify</filename>, <filename>pg_stat</filename>, <filename>pg_snapshots</filename> не ведётся расчёт контрольной суммы, равно как и для страниц, защищённых посредством полностраничной записи. Однако там, где такие структуры данных являются постоянными, записи WAL пишутся таким образом, чтобы после сбоя было возможно аккуратно повторить последние изменения, а эти записи WAL защищаются так же, как описано выше.</para>
    </listitem>
    <listitem>
     <para>Файлы каталога <filename>pg_twophase</filename> защищены с помощью контрольной суммы CRC-32.</para>
    </listitem>
    <listitem>
     <para>Временные файлы данных, используемые в больших SQL-запросах для сортировки, материализации и промежуточных результатов, в настоящее время не защищаются контрольной суммой, а изменения в этих файлах не отражаются в WAL.</para>
    </listitem>
   </itemizedlist></para>
  <para><productname>&productname;</productname> не защищает от корректируемых ошибок памяти; предполагается, что вы будете работать с памятью, которая использует промышленный стандарт коррекции ошибок (ECC, Error Correcting Codes) или лучшую защиту.</para>
 </sect1>

  <sect1 id="wal-intro">
   <title>Журнал упреждающей записи (<acronym>WAL</acronym>)</title>

   <indexterm zone="wal"><primary>WAL</primary></indexterm>

   <indexterm><primary>журнал транзакций</primary> <see>WAL</see></indexterm>

   <para><firstterm>Журнал упреждающей записи</firstterm> (<acronym>WAL</acronym>) — это стандартный метод обеспечения целостности данных. Детальное описание можно найти в большинстве книг (если не во всех) по обработке транзакций. Вкратце, основная идея <acronym>WAL</acronym> состоит в том, что изменения в файлах с данными (где находятся таблицы и индексы) должны записываться только после того, как эти изменения были занесены в журнал, т. е. после того как записи журнала, описывающие данные изменения, будут сохранены на постоянное устройство хранения. Если следовать этой процедуре, то записывать страницы данных на диск после подтверждения каждой транзакции нет необходимости, потому что мы знаем, что если случится сбой, то у нас будет возможность восстановить базу данных с помощью журнала: любые изменения, которые не были применены к страницам с данными, могут быть воссозданы из записей журнала. (Это называется восстановлением с воспроизведением, или REDO.)</para>

   <tip>
    <para>Поскольку <acronym>WAL</acronym> восстанавливает содержимое файлов базы данных, журналируемая файловая система не является необходимой для надёжного хранения файлов с данными или файлов WAL. Фактически, журналирование может снизить производительность, особенно если журналирование заставляет сохранять <emphasis>данные</emphasis> файловой системы на диск. К счастью, такое сохранение при журналировании часто можно отключить с помощью параметров монтирования файловой системы, например, <literal>data=writeback</literal> для файловой системы ext3 в Linux. С другой стороны, с журналируемыми файловыми системами увеличивается скорость загрузки после сбоя.</para>
   </tip>


   <para>Результатом использования <acronym>WAL</acronym> является значительное уменьшение количества запросов записи на диск, потому что для гарантии, что транзакция подтверждена, в записи на диск нуждается только файл журнала, а не каждый файл данных изменённый в результате транзакции. Файл журнала записывается последовательно и таким образом, затраты на синхронизацию журнала намного меньше, чем затраты на запись страниц с данными. Это особенно справедливо для серверов, которые обрабатывают много маленьких транзакций, изменяющих разные части хранилища данных. Таким образом, когда сервер обрабатывает множество мелких конкурентных транзакций, для подтверждения многих транзакций достаточно одного вызова <function>fsync</function> на файл журнала.</para>

   <para><acronym>WAL</acronym> также делает возможным поддержку онлайнового резервного копирования и восстановления на определённый момент времени, как описывается в <xref remap="6" linkend="continuous-archiving"/>. С помощью архивирования данных WAL поддерживается возврат к любому моменту времени, который доступен в данных WAL: мы просто устанавливаем предыдущую физическую резервную копию базы данных и воспроизводим журнал WAL до нужного момента времени. Более того, физическая резервная копия не должна быть мгновенным снимком состояния баз данных &mdash; если она была сделана некоторое время назад, воспроизведение журнала WAL за этот период исправит все внутренние несоответствия.</para>
  </sect1>

 <sect1 id="wal-async-commit">
  <title>Асинхронное подтверждение транзакций</title>

   <indexterm><primary>синхронная фиксация данных</primary></indexterm>

   <indexterm><primary>асинхронная фиксация данных</primary></indexterm>

  <para><firstterm>Асинхронная фиксация</firstterm> &mdash; это возможность завершать транзакции быстрее, ценой того, что в случае краха СУБД последние транзакции могут быть потеряны. Для многих приложений такой компромисс приемлем.</para>

  <para>Как описано в предыдущей части, подтверждение транзакции обычно <firstterm>синхронное</firstterm>: сервер ждёт сохранения записей <acronym>WAL</acronym> транзакции в постоянном хранилище, прежде чем сообщить клиенту об успешном завершении. Таким образом, клиенту гарантируется, что транзакция, которую подтвердил сервер, будет защищена, даже если сразу после этого произойдёт крах сервера. Однако, для коротких транзакций данная задержка будет основной составляющей общего времени транзакции. В режиме асинхронного подтверждения сервер сообщает об успешном завершении сразу, как только транзакция будет завершена логически, прежде чем сгенерированные записи <acronym>WAL</acronym> фактически будут записаны на диск. Это может значительно увеличить производительность при выполнении небольших транзакций.</para>

  <para>Асинхронное подтверждение транзакций приводит к риску потери данных. Существует короткое окно между отчётом о завершении транзакции для клиента и временем, когда транзакция реально подтверждена (т. е. гарантируется, что она не будет потеряна в случае краха сервера). Таким образом, асинхронное подтверждение транзакций не должно использоваться, если клиент будет выполнять внешние действия, опираясь на предположение, что транзакция будет сохранена. Например, банк конечно не должен использовать асинхронное подтверждение для транзакций в банкоматах, выдающих наличные. Но во многих случаях, таких как журналирование событий, столь серьёзная гарантия сохранности данных не нужна.</para>

  <para>Риск потери данных при использовании асинхронного подтверждения транзакций &mdash; это не риск повреждения данных. Если случился крах СУБД, она будет восстановлена путём воспроизведения <acronym>WAL</acronym> до последней записи, которая была записана на диск. Таким образом, будет восстановлено целостное состояние СУБД, но любые транзакции, которые ещё не были сохранены на диск, в этом состоянии не будут отражены. Чистый эффект будет заключаться в потере нескольких последних транзакций. Поскольку транзакции воспроизводятся в том же порядке, в котором подтверждались, воспроизведение не нарушает целостность &mdash; например, если транзакция "B" выполнила изменения, которые влияют на предыдущую транзакцию "A", то не может быть такого, что изменения, выполненные "A" были потеряны, а изменения, внесённые "B" сохранены.</para>

  <para>Пользователь может выбрать режим подтверждения для каждой транзакции, так что возможен конкурентный запуск транзакций в синхронном и асинхронном режиме. Это позволяет достичь гибкого компромисса между производительностью и конечно надёжностью транзакций. Режим подтверждения транзакций управляется параметром <xref linkend="guc-synchronous-commit"/>, который может быть изменён любым из способов, пригодным для установки параметров конфигурации. Режим, используемый для какой-либо конкретной транзакции, зависит от значения <varname>synchronous_commit</varname>, которое действует на момент начала этой транзакции.</para>

  <para>Некоторые команды, например <command>DROP TABLE</command>, принудительно запускают синхронное подтверждение транзакции, независимо от значения <varname>synchronous_commit</varname>. Это сделано для того, чтобы иметь уверенность в целостности данных между файловой системой сервера и логическим состоянием базы данных. Команды, которые поддерживают двухфазное подтверждение транзакций, такие как <command>PREPARE TRANSACTION</command>, также всегда синхронные.</para>

  <para>Если во время окна риска между асинхронным подтверждением транзакции и сохранением на диск записей <acronym>WAL</acronym>, происходит крах СУБД, то изменения, сделанные во время этой транзакции <emphasis>будут</emphasis> потеряны. Продолжительность окна риска ограничена, потому что фоновый процесс (<quote>WAL writer</quote>), сохраняет не записанные записи <acronym>WAL</acronym> на диск каждые <xref linkend="guc-wal-writer-delay"/> миллисекунд. Фактически, максимальная продолжительность окна риска составляет трёхкратное значение <varname>wal_writer_delay</varname>, потому что WAL writer разработан так, чтобы сразу сохранять целые страницы во время периодов занятости.</para>

  <caution>
   <para>Режим немедленного завершения работы (immediate) эквивалентен краху сервера и приведёт, таким образом, к потере всех не сохранённых асинхронных транзакций.</para>
  </caution>

  <para>Асинхронное подтверждение транзакций предоставляет поведение, которое отличается от того, что соответствует установке параметра <xref linkend="guc-fsync"/> = off. Настройка <varname>fsync</varname> касается всего сервера и может изменить поведение всех транзакций. Она выключает всю логику внутри <productname>&productname;</productname>, которая пытается синхронизировать запись отдельных порций в базу данных и, таким образом, крах системы (обусловленный отказом аппаратного обеспечения или операционной системы, который не является сбоем самой СУБД <productname>&productname;</productname> ) может в результате привести к повреждению состояния базы данных. Во многих случаях, асинхронное подтверждение транзакций предоставляет лучшую производительность, чем то, что можно получить выключением <varname>fsync</varname>, но без риска повреждения данных.</para>

  <para><xref linkend="guc-commit-delay"/> также выглядит очень похоже на асинхронное подтверждение транзакций, но по сути это является методом асинхронного подтверждения транзакций (фактически, во время асинхронных транзакций <varname>commit_delay</varname> игнорируется). <varname>commit_delay</varname> приводит к задержке только перед тем, как синхронная транзакция пытается записать данные <acronym>WAL</acronym> на диск, в надежде, что одиночная запись, выполняемая на одну такую транзакцию, сможет также обслужить другие транзакции, которые подтверждаются приблизительно в это же время. Установку этого параметра можно рассматривать как способ увеличения промежутка времени, в течение которого транзакции группируются для единовременной записи на диск. Это распределяет стоимость записи между несколькими транзакциями.</para>

 </sect1>

 <sect1 id="wal-configuration">
  <title>Настройка <acronym>WAL</acronym></title>

  <para>Существует несколько конфигурационных параметров относящихся к <acronym>WAL</acronym>, которые влияют на производительность СУБД. Далее рассказывается об их использовании. Общую информацию об установке параметров конфигурации сервера смотрите в <xref remap="6" linkend="runtime-config"/>.</para>

  <para><firstterm>Контрольные точки</firstterm><indexterm><primary>контрольная точка</primary></indexterm> &mdash; это точки в последовательности транзакций, в которых гарантируется, что файлы с данными и индексами были обновлены всей информацией записанной перед контрольной точкой. Во время контрольной точки, все страницы данных, находящиеся в памяти, сохраняются на диск, а в файл журнала записывается специальная запись контрольной точки. (Сделанные изменения были перед этим записаны в файлы <acronym>WAL</acronym>.) В случае краха процедура восстановления ищет последнюю запись контрольной точки, чтобы определить эту точку в журнале (называемую записью REDO), от которой процедура должна начать операцию воспроизведения изменений. Любые изменения файлов данных перед этой точкой гарантированно находятся уже на диске. Таким образом, после контрольной точки, сегменты журнала, которые предшествуют записи воспроизведения, больше не нужны и могут быть удалены или пущены в циклическую перезапись. (Когда архивирование WAL будет завершено, сегменты журнала должны быть архивированы перед их удалением или циклической перезаписи.)</para>

  <para>Запись всех страниц данных из памяти на диск, которая требуется для контрольной точки, может вызвать значительную нагрузку на дисковый ввод/вывод. По этой причине, активность записи по контрольной точке регулируется так, что ввод/вывод начинается при старте контрольной точки и завершается перед стартом следующей контрольной точки; это минимизирует потерю производительности во время прохождения контрольных точек.</para>

  <para>Отдельный серверный процесс контрольных точек автоматически выполняет контрольные точки с заданной частотой. Контрольные точки производятся каждые <xref linkend="guc-checkpoint-timeout"/> секунд либо при приближении к пределу <xref linkend="guc-max-wal-size"/>, если это имеет место раньше. Значения по умолчанию: 5 минут и 1 Гбайт, соответственно. Если после предыдущей контрольной точки новые записи WAL не добавились, следующие контрольные точки будут пропущены, даже если проходит время <varname>checkpoint_timeout</varname>. (Если вы применяете архивацию WAL и хотите установить нижний предел для частоты архивации, чтобы ограничить потенциальную потерю данных, вам следует настраивать параметр <xref linkend="guc-archive-timeout"/>, а не параметры контрольных точек.) Также можно выполнить контрольную точку принудительно, воспользовавшись SQL-командой <command>CHECKPOINT</command>.</para>

  <para>Уменьшение значений <varname>checkpoint_timeout</varname> и/или <varname>max_wal_size</varname> приводит к учащению контрольных точек. Это позволяет ускорить восстановление после краха (поскольку для воспроизведения нужно меньше данных), но с другой стороны нужно учитывать дополнительную нагрузку, возникающую вследствие более частого сброса изменённых страниц данных на диск. Если включён режим <xref linkend="guc-full-page-writes"/> (по умолчанию это так), нужно учесть и ещё один фактор. Для обеспечения целостности страницы данных, при первом изменении страницы данных после контрольной точки эта страница записывается в журнал целиком. В данном случае, чем меньше интервал между контрольными точками, тем больше объём записи в журнал WAL, так что это частично дискредитирует идею уменьшения интервала записи, и в любом случае приводит к увеличению объёма обмена с диском</para>

  <para>Контрольные точки довольно дороги с точки зрения ресурсов, во-первых, потому что они требуют записи всех буферов из памяти на диск, и во-вторых потому что они создают дополнительный трафик WAL, о чём говорилось выше. Таким образом, будет благоразумным установить параметры контрольных точек так, чтобы контрольные точки не выполнялись слишком часто. Для простой проверки параметров контрольной точки можно установить параметр <xref linkend="guc-checkpoint-warning"/>. Если промежуток времени между контрольными точками будет меньше чем количество секунд, заданное параметром <varname>checkpoint_warning</varname>, то в журнал сервера будет выдано сообщение с рекомендацией увеличить <varname>max_wal_size</varname>. Эпизодическое появление такого сообщения не является поводом для беспокойства. Но если оно появляется часто, необходимо увеличить значения параметров управления контрольными точками. Массовые операции, такие как <command>COPY</command> с большим объёмом данных, могут привести к появлению нескольких таких предупреждений, если вы не установили <varname>max_wal_size</varname> достаточно большим.</para>

  <para>Чтобы избежать &laquo;заваливания&raquo; системы ввода/вывода при резкой интенсивной записи страниц, запись &laquo;грязных&raquo; буферов во время контрольной точки растягивается на определённый период времени. Этот период управляется параметром <xref linkend="guc-checkpoint-completion-target"/>, который задаётся как часть интервала контрольной точки. Скорость ввода/вывода подстраивается так, чтобы контрольная точка завершилась к моменту истечения заданной части от <varname>checkpoint_timeout</varname> секунд или до превышения <varname>max_wal_size</varname>, если оно имеет место раньше. Со значением 0.5, заданным по умолчанию, можно ожидать, что <productname>&productname;</productname> завершит процедуру контрольной точки примерно за половину времени до начала следующей. В системе, которая работает практически на пределе мощности ввода/вывода в обычном режиме, есть смысл увеличить <varname>checkpoint_completion_target</varname>, чтобы уменьшить нагрузку ввода/вывода, связанную с контрольными точками. Но с другой стороны, растягивание контрольных точек влияет на время восстановления, так как для восстановления нужно будет задействовать большее количество сегментов WAL. Хотя в <varname>checkpoint_completion_target</varname> можно задать значение вплоть до 1.0, лучше выбрать значение меньше (по крайней мере, не больше 0.9), так как при контрольных точках выполняются и некоторые другие операции, помимо записи &laquo;грязных&raquo; буферов. Со значением 1.0 контрольные точки, скорее всего, не будут завершаться вовремя, что приведёт к потере производительности из-за неожиданных колебаний требуемого количества сегментов WAL.</para>

  <para>На платформах Linux и POSIX параметр <xref linkend="guc-checkpoint-flush-after"/> позволяет принудить ОС к сбросу страниц, записываемых во время контрольной точки, при накоплении заданного количества байт. Если его не настроить, эти страницы могут оставаться в кеше страниц ОС, что повлечёт затормаживание при выполнении <literal>fsync</literal> в конце контрольной точки. Этот параметр часто помогает уменьшить задержки транзакций, но может оказать и негативное влияние на производительность; особенно, когда объём нагрузки больше <xref linkend="guc-shared-buffers"/>, но меньше кеша страниц в ОС.</para>

  <para>Число файлов сегментов WAL в каталоге <filename>pg_xlog</filename> зависит от <varname>min_wal_size</varname>, <varname>max_wal_size</varname> и объёма WAL, сгенерированного в предыдущих циклах контрольных точек. Когда старые файлы сегментов оказываются не нужны, они удаляются или перерабатываются (то есть переименовываются, чтобы стать будущими сегментами в нумерованной последовательности). Если вследствие кратковременного скачка интенсивности записи в журнал, предел <varname>max_wal_size</varname> превышается, ненужные файлы сегментов будут удаляться, пока система не опустится ниже этого предела. Оставаясь ниже этого предела, система перерабатывает столько файлов WAL, сколько необходимо для покрытия ожидаемой потребности до следующей контрольной точки, и удаляет остальные. Эта оценка базируется на скользящем среднем числа файлов WAL, задействованных в предыдущих циклах контрольных точек. Скользящее среднее увеличивается немедленно, если фактическое использование превышает оценку, так что в нём в некоторой степени накапливается пиковое использование, а не среднее. Значение <varname>min_wal_size</varname> ограничивает снизу число файлов WAL, которые будут переработаны для будущего использования; такой объём WAL всегда будет перерабатываться, даже если система простаивает и оценка использования говорит, что нужен совсем небольшой WAL.</para>

  <para>Вне зависимости от <varname>max_wal_size</varname>, в количестве <xref linkend="guc-wal-keep-segments"/> + 1 самые последние файлы WAL сохраняются в любом случае. Так же, если применяется архивация WAL, старые сегменты не могут быть удалены или переработаны, пока они не будут заархивированы. Если WAL архивируется медленнее, чем генерируется, либо если команда <varname>archive_command</varname> постоянно даёт сбои, старые файлы WAL будут накапливаться в <filename>pg_xlog</filename>, пока ситуация не будет разрешена. Медленно работающий или отказавший резервный сервер, использующий слот репликации, даст тот же эффект (см. <xref remap="4" linkend="streaming-replication-slots"/>).</para>

  <para>В режиме восстановления архива или горячего резерва сервер периодически выполняет <firstterm>точки перезапуска</firstterm><indexterm><primary>точка перезапуска</primary></indexterm>, которые похожи на контрольные точки в обычном режиме работы: сервер принудительно сбрасывает своё состояние на диск, обновляет файл <filename>pg_control</filename>, чтобы показать, что уже обработанные данные WAL не нужно сканировать снова, и затем перерабатывает все старые файлы сегментов журнала в каталоге <filename>pg_xlog</filename>. Точки перезапуска не могут выполняться чаще, чем контрольные точки на главном сервере, так как они могут происходить только в записях контрольных точек. Точка перезапуска производится, когда достигается запись контрольной точки и после предыдущей точки перезапуска прошло не меньше <varname>checkpoint_timeout</varname> секунд или размер WAL может превысить <varname>max_wal_size</varname>. Однако из-за того, что на время выполнения точек перезапуска накладываются ограничения, <varname>max_wal_size</varname> часто превышается при восстановлении, вплоть до объёма WAL, записываемого в цикле между контрольными точками. (Значение <varname>max_wal_size</varname> никогда и не было жёстким пределом, так что всегда следует оставлять приличный запас сверху, чтобы не остаться без свободного места на диске.)</para>

  <para>Наиболее часто используются две связанные с <acronym>WAL</acronym> внутренние функции: <function>XLogInsertRecord</function> и <function>XLogFlush</function>. <function>XLogInsertRecord</function> применяется для добавления записи в буферы <acronym>WAL</acronym> в разделяемой памяти. Если места для новой записи недостаточно, <function>XLogInsertRecord</function> придётся записать (переместить в кеш ядра) несколько заполненных буферов <acronym>WAL</acronym>. Это нежелательно, так как <function>XLogInsertRecord</function> используется при каждом изменении в базе данных на низком уровне (например, при добавлении строки) в момент, когда установлена исключительная блокировка задействованных страниц данных, поэтому данная операция должна быть максимально быстрой. Что ещё хуже, запись буферов <acronym>WAL</acronym> может также повлечь создание нового сегмента журнала, что займёт ещё больше времени. Обычно буферы <acronym>WAL</acronym> должны записываться и сохраняться на диске в функции <function>XLogFlush</function>, которая вызывается, по большей части, при фиксировании транзакции, чтобы результаты транзакции сохранились в надёжном хранилище. В системах с интенсивной записью в журнал вызовы <function>XLogFlush</function> могут иметь место не так часто, чтобы <function>XLogInsertRecord</function> не приходилось производить запись. В таких системах следует увеличить число буферов <acronym>WAL</acronym>, изменив параметр <xref linkend="guc-wal-buffers"/>. Когда включён режим <xref linkend="guc-full-page-writes"/> и система очень сильно загружена, увеличение <varname>wal_buffers</varname> поможет сгладить скачки во времени ответа в период сразу после каждой контрольной точки.</para>

  <para>Параметр <xref linkend="guc-commit-delay"/> определяет, на сколько микросекунд будет засыпать ведущий процесс группы, записывающий в журнал, после получения блокировки в <function>XLogFlush</function>, пока подчинённые формируют очередь на запись. Во время этой задержки другие серверные процессы смогут добавлять записи в WAL буферы журнала, чтобы все эти записи сохранились на диск в результате одной операции синхронизации, которую выполнит ведущий. Ведущий процесс не засыпает, если отключён режим <xref linkend="guc-fsync"/>, либо число сеансов с активными транзакциями меньше <xref linkend="guc-commit-siblings"/>, так как маловероятно, что какой-либо другой сеанс зафиксирует транзакцию в ближайшее время. Заметьте, что на некоторых платформах, разрешение этого таймера сна составляет 10 миллисекунд, так что любое значение параметра <varname>commit_delay</varname> от 1 до 10000 микросекунд будет действовать одинаково. Кроме того, в некоторых системах состояние сна может продлиться несколько дольше, чем требует параметр.</para>

  <para>Так как цель <varname>commit_delay</varname> состоит в том, чтобы позволить стоимости каждой операции синхронизации амортизироваться через параллельную фиксацию транзакций (потенциально за счёт задержки транзакции), необходимо определить количество той стоимости, прежде чем урегулирование сможет быть выбрано разумно. Чем выше стоимость, тем более эффективный будет <varname>commit_delay</varname> в увеличении пропускной способности транзакций в какой-то степени. Программа <xref linkend="pgtestfsync"/> может использоваться, чтобы измерить среднее время в микросекундах, которое занимает одиночная работа сброса WAL на диск. Значение половины среднего времени сообщаемого программой рекомендуется в качестве отправной точки для использования значения в параметре <varname>commit_delay</varname> при оптимизации для конкретного объёма работы, и говорит о том, сколько нужно времени для синхронизации сброса единственной операции записи 8 КБ. Настройка параметра <varname>commit_delay</varname> особенно полезна в случае хранения WAL в хранилище с высокоскоростными дисками, такими как твердотельные накопители (SSD) или RAID-массивы с кешем записи и аварийным питанием на батарее; но это определённо должно тестироваться на репрезентативной рабочей нагрузке. Более высокие значения <varname>commit_siblings</varname> должны использоваться в таких случаях, тогда как меньшие значения <varname>commit_siblings</varname> часто полезны на носителях с большими задержками. Обратите внимание на то, что увеличение значения параметра <varname>commit_delay</varname> может увеличить задержку транзакции настолько, что пострадает общая производительность транзакций.</para>

  <para>Даже если <varname>commit_delay</varname> равен нулю (значение по умолчанию), групповая фиксация все равно может произойти, но группа будет состоять только из тех сеансов, которым понадобилось сбросить записи о фиксации на диск за то время, пока происходил предыдущий сброс. Чем больше сеансов, тем чаще это происходит даже при нулевом <varname>commit_delay</varname>, поэтому увеличение этого параметра может и не оказать заметного действия. Установка <varname>commit_delay</varname> имеет смысл в двух случаях: (1) когда несколько транзакций одновременно фиксируют изменения, (2) либо когда частота фиксаций ограничена пропускной способностью дисковой подсистемы. Однако при задержке из-за низкой скорости вращения диска, эта настройка может оказаться полезной даже всего при двух сеансах.</para>

  <para>Параметр <xref linkend="guc-wal-sync-method"/> определяет, как <productname>&productname;</productname> будет обращаться к ядру, чтобы принудительно сохранить <acronym>WAL</acronym> на диск. Все методы должны быть одинаковыми в плане надёжности, за исключением <literal>fsync_writethrough</literal>, который может иногда принудительно сбрасывать кеш диска, даже если другие методы не делают этого. Однако, какой из них самый быстрый, во многом определяется платформой; вы можете протестировать скорость, используя модуль <xref linkend="pgtestfsync"/>. Обратите внимание, что данный параметр не имеет значения, если <varname>fsync</varname> выключен.</para>

  <para>Включение параметра конфигурации <xref linkend="guc-wal-debug"/> (предоставляется, если <productname>&productname;</productname> был скомпилирован с его поддержкой) будет приводить к тому, что все вызовы связанных с <acronym>WAL</acronym> функций <function>XLogInsertRecord</function> и <function>XLogFlush</function> будут протоколироваться в журнале сервера. В будущем данный параметр может быть заменён более общим механизмом.</para>
 </sect1>

 <sect1 id="wal-internals">
  <title>Внутреннее устройство WAL</title>

  <para><acronym>WAL</acronym> включается автоматически; от администратора не требуется никаких действий за исключением того, чтобы убедиться, что выполнены требования <acronym>WAL</acronym> к месту на диске, и что выполнены все необходимые действия по тонкой настройке (см. <xref remap="4" linkend="wal-configuration"/>).</para>

  <para>Журналы <acronym>WAL</acronym> хранятся в каталоге <filename>pg_xlog</filename>, который находится в каталоге data, в виде списка файлов сегментов, обычно 16MB каждый (этот размер может быть изменён с помощью указания configure <option>--with-wal-segsize</option> во время компиляции сервера). Каждый файл сегмента разделяется на страницы, обычно по 8KB каждая (данный размер может быть изменён с помощью указания configure <option>--with-wal-blocksize</option>). Заголовки записи журнала описываются в <filename>access/xlogrecord.h</filename>; содержимое самой записи зависит от типа события, которое сохраняется в журнале. Файлы сегментов имеют имена-номера, которые начинаются с <filename>000000010000000000000000</filename> и увеличиваются автоматически. Эти номера не перекрываются, но пока доступные номера исчерпаются, пройдёт очень, очень долгое время.</para>

  <para>Выгодно размещать журналы WAL на другом диске, отличном от того, где находятся основные файлы базы данных. Для этого можно переместить каталог <filename>pg_xlog</filename> в другое место (разумеется, когда сервер остановлен) и создать символьную ссылку из исходного места на перемещённый каталог.</para>

  <para>Для <acronym>WAL</acronym> важно, чтобы запись в журнал выполнялась до изменений данных в базе. Но этот порядок могут нарушить дисковые устройства<indexterm><primary>дисковое устройство</primary></indexterm>, которые ложно сообщают ядру об успешном завершении записи, хотя фактически они только выполнили кеширование данных и пока не сохранили их на диск. Сбой питания в такой ситуации может привести к неисправимому повреждению данных. Администраторы должны убедиться, что диски, где хранятся файлы журналов <acronym>WAL</acronym> <productname>&productname;</productname>, не выдают таких ложных сообщений ядру. (См. <xref remap="4" linkend="wal-reliability"/>.)</para>

  <para>После выполнения контрольной точки и сброса журнала позиция контрольной точки сохраняется в файл <filename>pg_control</filename>. Таким образом, при старте восстановления сервер сперва читает файл <filename>pg_control</filename> и затем запись контрольной точки; затем он выполняет операцию REDO, сканируя вперёд от точки журнала, обозначенной в записи контрольной точки. Поскольку полное содержимое страниц данных сохраняется в журнале в первой странице после контрольной точки (предполагается, что включён режим <xref linkend="guc-full-page-writes"/>), все страницы, изменённые с момента контрольной точки, будут восстановлены в целостном состоянии.</para>

  <para>В случае, если файл <filename>pg_control</filename> повреждён, мы должны поддерживать возможность сканирования существующих сегментов журнала в обратном порядке &mdash; от новых к старым &mdash; чтобы найти последнюю контрольную точку. Это пока не реализовано. <filename>pg_control</filename> является достаточно маленьким файлом (меньше, чем одна дисковая страница), который не должен попадать под проблему частичной записи и на момент написания данной документации, не было ни одного сообщения о сбоях СУБД исключительно из-за невозможности чтения самого файла <filename>pg_control</filename>. Таким образом, хотя теоретически это является слабым местом, на практике проблем с <filename>pg_control</filename> не обнаружено.</para>
 </sect1>
</chapter>
