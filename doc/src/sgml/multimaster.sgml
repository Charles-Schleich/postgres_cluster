<sect1 id="multimaster">
  <title>multimaster</title>
      <para><emphasis role="strong">Table of Contents</emphasis></para>
      <para><link linkend="multimaster-limitations">Limitations</link></para>
      <para><link linkend="multimaster-architecture">Architecture</link></para>
      <para><link linkend="multimaster-installation">Installation and Setup</link></para>
      <para><link linkend="multimaster-administration">Multi-Master Cluster Administration</link></para>
      <para><link linkend="multimaster-reference">Reference</link></para>
      <para><link linkend="multimaster-compatibility">Compatibility</link></para>
      <para><link linkend="multimaster-authors">Authors</link></para>
  <para>
    <filename>multimaster</filename> is a <productname>&productname;</productname> extension with a set
    of patches that turns <productname>&productname;</productname> into a synchronous shared-nothing
    cluster to provide Online Transaction Processing (<acronym>OLTP</acronym>) scalability for read transactions and high availability with automatic disaster recovery.</para>
    <para> As compared to a standard <productname>PostgreSQL</productname> master-slave cluster, a cluster configured with the <filename>multimaster</filename> extension offers the following benefits:</para>
    <itemizedlist>
      <listitem>
        <para>
          Cluster-wide transaction isolation
        </para>
      </listitem>
      <listitem>
        <para>
          Synchronous logical replication and DDL replication
        </para>
      </listitem>
      <listitem>
        <para>
         Working with temporary tables on each cluster node
        </para>
      </listitem>
      <listitem>
        <para>
          Fault tolerance and automatic node recovery
        </para>
      </listitem>
      <listitem>
        <para>
        <productname>PostgreSQL</productname> online upgrades 
        </para>
      </listitem>
    </itemizedlist>
    <para>
      The <filename>multimaster</filename> extension replicates your
      database to all nodes of the cluster and allows write transactions
      on each node. To ensure data consistency in the case of concurrent
      updates, <filename>multimaster</filename> enforces transaction
      isolation cluster-wide, using multiversion concurrency control
      (<acronym>MVCC</acronym>) at the <link linkend="xact-read-committed">read committed</link> or <link linkend="xact-repeatable-read">repeatable read</link> isolation levels. Any write
      transaction is synchronously replicated to all nodes, which
      increases commit latency for the time required for
      synchronization. Read-only transactions and queries are executed
      locally, without any measurable overhead.
    </para>
    <para>
      To ensure high availability and fault tolerance of the cluster,
      <filename>multimaster</filename> uses three-phase commit protocol
      and heartbeats for failure discovery. A multi-master cluster of <replaceable>N</replaceable>
      nodes can continue working while the majority of the nodes are
      alive and reachable by other nodes. To be configured with <filename>multimaster</filename>, the cluster must include at least two nodes. In most cases, three 
      cluster nodes are enough to ensure high availability. Since the data on all cluster nodes is the same, you do not typically need more than five cluster nodes.</para>
      <para>When a failed node 
      is reconnected to the cluster, <filename>multimaster</filename> can automatically
      fast-forward the node to the actual state based on the
      Write-Ahead Log (<acronym>WAL</acronym>) data in the corresponding replication slot. If <acronym>WAL</acronym> data is no longer available for the time when the node was excluded from the cluster, you can <link linkend="multimaster-restoring-a-node-manually">restore the node using <application>pg_basebackup</application></link>.
    </para>
    <important><para>When using <filename>multimaster</filename>, make sure to take its replication restrictions into account. For details, see <xref linkend="multimaster-limitations">.</para></important>
    <para>
      To learn more about the <filename>multimaster</filename> internals, see
      <xref linkend="multimaster-architecture">.
    </para>
    
  <sect2 id="multimaster-limitations">
    <title>Limitations</title>
    <para>The <filename>multimaster</filename> extension takes care of the database replication in a fully automated way. You can perform write transactions on any node and work with temporary tables on each cluster node simultaneously. However, make sure to take the following replication restrictions into account:</para>
    <itemizedlist>
      <listitem>
        <para>
          <filename>multimaster</filename> can only replicate one database
          per cluster, which is specified in the <varname>multimaster.conn_strings</varname> variable. If you try to connect to a different database, <filename>multimaster</filename> will return a corresponding error message.
        </para>
      </listitem>
      <listitem>
        <para>
          The replicated tables must have primary keys or replica identity. Otherwise, 
          <filename>multimaster</filename> will not allow replication 
          because of the logical replication restrictions. Unlogged tables are not replicated, as in the standard <productname>PostgreSQL</productname>.
        </para>
        <note><para>You can enable replication
of tables without primary keys by setting the <varname>multimaster.ignore_tables_without_pk</varname> variable to <literal>false</literal>. However, take into account that
<filename>multimaster</filename> does not allow update operations on such tables.</para></note>
      </listitem>
      <listitem>
        <para>
          Isolation level. The <filename>multimaster</filename> extension
          supports <emphasis><literal>read committed</literal></emphasis> and <emphasis><literal>repeatable read</literal></emphasis> isolation levels. <emphasis><literal>Serializable</literal></emphasis> isolation level is currently not supported.</para>
        <important>
        <para>Using <literal>repeatable read</literal> isolation level increases
          the probability of serialization failure at commit time. Unlike in the standard <productname>PostgreSQL</productname>, <literal>read committed</literal> level can also cause serialization failures on a multi-master cluster.</para>
          <para>When performing a write transaction, <filename>multimaster</filename> blocks the affected objects only on the node on which the transaction is performed. However, since write transactions are allowed on all nodes, other transactions can try to change the same objects on the neighbor nodes at the same time. In this case, the replication of the first transaction can fail because the affected objects on the neighbor nodes are already blocked by another transaction. Similarly, the latter transaction cannot be replicated to the first node. In this case, a distributed deadlock occurs. As a result, one of the transactions is automatically rolled back and needs to be repeated. The application must be ready to retry transactions.
  </para>
    <para>If your typical workload has too many rollbacks, it is recommended to use <literal>read committed</literal> isolation level. However, the <literal>read committed</literal> still does not guarantee the absence of deadlocks on a multi-master cluster. If using the <literal>read committed</literal> level does not help, you can try directing all the write transactions to a single node.</para>
    </important>
      </listitem>
      <listitem>
        <para>
          Sequence generation. To avoid conflicts between unique identifiers on different nodes, <filename>multimaster</filename> modifies the default behavior of sequence generators. For each node, ID generation is started with the node number and is incremented by the number of nodes. For example, in a three-node cluster, 1, 4, and 7 IDs are allocated to the objects written onto the first node, while 2, 5, and 8 IDs are reserved for the second node. If you change the number of nodes in the cluster, the incrementation interval for new IDs is adjusted accordingly. Thus, the generated sequence values are not monotonic.
        </para>
      </listitem>
      <listitem>
        <para>
          <acronym>DDL</acronym> replication. While <filename>multimaster</filename>
          replicates data on the logical level, <acronym>DDL</acronym> is replicated on the
          statement level, which results in distributed commits of the same
          statement on different nodes. As a result, complex <acronym>DDL</acronym>
          scenarios, such as stored procedures and temporary tables, may
          work differently as compared to the standard <productname>PostgreSQL</productname>.
        </para>
      </listitem>
      <listitem>
        <para>
          Commit latency. In the current implementation of logical
          replication, <filename>multimaster</filename> sends data to subscriber nodes only after the
          local commit, so you have to wait for transaction processing twice: first on the local node,
          and then on all the other nodes simultaneously. In the case of a heavy-write transaction, this may result in a noticeable delay.
        </para>
      </listitem>
    </itemizedlist>
<para>If you have any data that must be present on one of the nodes only, you can exclude a particular table from replication, as follows:
    <programlisting>SELECT * FROM <function>mtm.make_table_local</function>('table_name') </programlisting> 
    </para>
  </sect2>
    
    <sect2 id="multimaster-architecture">
  <title>Architecture</title>
  <sect3 id="multimaster-replication">
    <title>Replication</title>
    <para>
      Since each server in a multi-master cluster can accept writes, any server can abort a
      transaction because of a concurrent update &mdash; in the same way as it
      happens on a single server between different backends. To ensure
      high availability and data consistency on all cluster nodes,
      <filename>multimaster</filename> uses <link linkend="logicaldecoding-synchronous">logical replication</link> and the <link linkend="multimaster-credits">three-phase E3PC commit protocol</link>.
    </para>
    <para>
      When <productname>&productname;</productname> loads the <filename>multimaster</filename> shared
      library, <filename>multimaster</filename> sets up a logical
      replication producer and consumer for each node, and hooks into
      the transaction commit pipeline. The typical data replication
      workflow consists of the following phases:
    </para>
    <orderedlist>
      <listitem>
        <para>
          <literal>PREPARE</literal> phase.
          <filename>multimaster</filename> captures and implicitly
          transforms each <literal>COMMIT</literal> statement to a
          <literal>PREPARE</literal> statement. All the nodes that get
          the transaction via the replication protocol (<emphasis>the
          cohort nodes</emphasis>) send their vote for approving or
          declining the transaction to the arbiter process on the
          initiating node. This ensures that all the cohort can accept
          the transaction, and no write conflicts occur. For details on
          <literal>PREPARE</literal> transactions support in <productname>PostgreSQL</productname>,
          see the
          <link linkend="sql-prepare-transaction">PREPARE
          TRANSACTION</link> topic.
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>PRECOMMIT</literal> phase. If all the cohort nodes approve
          the transaction, the arbiter process sends a
          <literal>PRECOMMIT</literal> message to all the cohort nodes
          to express an intention to commit the transaction. The cohort
          nodes respond to the arbiter with the
          <literal>PRECOMMITTED</literal> message. In case of a failure,
          all the nodes can use this information to complete the
          transaction using a quorum-based voting procedure.
        </para>
      </listitem>
      <listitem>
        <para>
          <literal>COMMIT</literal> phase. If
          <literal>PRECOMMIT</literal> is successful, the arbiter
          commits the transaction to all nodes.
        </para>
      </listitem>
    </orderedlist>
    <important>
        <para><filename>multimaster</filename> currently supports the <literal>read committed</literal> and <literal>repeatable read</literal> isolation levels only, which can cause unexpected serialization failures in your workload. For details, see <xref linkend="multimaster-limitations">.</para>
    </important>
    <para>
      If a node crashes or gets disconnected from the cluster between
      the <literal>PREPARE</literal> and <literal>COMMIT</literal>
      phases, the <literal>PRECOMMIT</literal> phase ensures that the
      survived nodes have enough information to complete the prepared
      transaction. The <literal>PRECOMMITTED</literal> messages help
      avoid the situation when the crashed node has already committed
      or aborted the transaction, but has not notified other nodes
      about the transaction status. In a two-phase commit (2PC), such a
      transaction would block resources (hold locks) until the recovery
      of the crashed node. Otherwise, you could get data inconsistencies
      in the database when the failed node is recovered. For example, if
      the failed node committed the transaction but the survived node
      aborted it.
    </para>
    <para>
      To complete the transaction, the arbiter must receive a response
      from the majority of the nodes. For example, for a cluster of 2<replaceable>N</replaceable>+1 nodes, at least <replaceable>N</replaceable>+1 responses are required. Thus, <filename>multimaster</filename> ensures that
      your cluster is available for reads and writes while the majority
      of the nodes are connected, and no data inconsistencies occur in
      case of a node or connection failure. For details on the failure
      detection mechanism, see
      <xref linkend="multimaster-failure-detection-and-recovery">.
    </para>
  </sect3>
  <sect3 id="multimaster-failure-detection-and-recovery">
    <title>Failure Detection and Recovery</title>
    <para>
      Since <filename>multimaster</filename> allows writes to each node,
      it has to wait for responses about transaction acknowledgement
      from all the other nodes. Without special actions in case of a
      node failure, each commit would have to wait until the failed node
      recovery. To deal with such situations,
      <filename>multimaster</filename> periodically sends heartbeats to
      check the node state and the connectivity between nodes. When several
      heartbeats to the node are lost in a row, this node is kicked out
      of the cluster to allow writes to the remaining alive nodes. You
      can configure the heartbeat frequency and the response timeout in
      the <literal>multimaster.heartbeat_send_timeout</literal> and
      <literal>multimaster.heartbeat_recv_timeout</literal> parameters,
      respectively.
    </para>
    <para>
      For alive nodes, there is no way to distinguish between a failed
      node that stopped serving requests and a network-partitioned node
      that can be accessed by database users, but is unreachable for
      other nodes. To avoid conflicting writes to nodes in different
      network partitions, <filename>multimaster</filename> only allows
      writes to the nodes that see the majority of other nodes.
    </para>
    <para>
      For example, suppose a five-node multi-master cluster experienced
      a network failure that split the network into two isolated
      subnets, with two and three cluster nodes. Based on heartbeats
      propagation information, <filename>multimaster</filename> will
      continue accepting writes at each node in the bigger partition,
      and deny all writes in the smaller one. Thus, a cluster consisting
      of 2<replaceable>N</replaceable>+1 nodes can tolerate <replaceable>N</replaceable> node failures and stay alive if any
      <replaceable>N</replaceable>+1 nodes are alive and connected to each other.
    </para>
    <para>
      In case of a partial network split when different nodes have
      different connectivity, <filename>multimaster</filename> finds a
      fully connected subset of nodes and switches off other nodes. For
      example, in a three-node cluster, if node A can access both B and
      C, but node B cannot access node C, <filename>multimaster</filename>
      isolates node C to ensure data consistency on nodes A and B.
    </para>
    <para>
      Each node maintains a data structure that keeps the information about the state of all
      nodes in relation to this node. You can get this data in the
      <literal>mtm.get_nodes_state()</literal> view.
    </para>
    <para>
      When a failed node connects back to the cluster, 
      <filename>multimaster</filename> starts automatic recovery:
    </para>
    <orderedlist>
      <listitem>
        <para>
          The reconnected node selects a random cluster node and starts
          catching up with the current state of the cluster based on the
          Write-Ahead Log (WAL).
        </para>
      </listitem>
      <listitem>
        <para>
          When the node gets synchronized up to the minimum recovery
          lag, all the cluster nodes get locked for write transactions to allow the
          recovery process to finish. By default, the minimum recovery
          lag is 100KB. You can change this value in the
          <varname>multimaster.min_recovery_lag</varname> variable.
        </para>
      </listitem>
      <listitem>
        <para>
          When the recovery is complete, <filename>multimaster</filename>
          promotes the reconnected node to the online state and
          includes it into the replication scheme.
        </para>
      </listitem>
    </orderedlist>
    <note><para>
      Automatic recovery is only possible if the failed node WAL lag
      behind the working ones does not exceed the
      <varname>multimaster.max_recovery_lag</varname> value. When the
      WAL lag is bigger than
      <varname>multimaster.max_recovery_lag</varname>, you can manually
      restore the node from one of the working nodes using
      <application>pg_basebackup</application>.
    </para></note>
    <para><emphasis role="strong">See Also</emphasis></para>
    <para><link linkend="multimaster-restoring-a-node-manually">Restoring a Cluster Node</link></para>
  </sect3>
</sect2>
  <sect2 id="multimaster-installation">
    <title>Installation and Setup</title>
      <para>
        To use <filename>multimaster</filename>, you need to install
        <productname>&productname;</productname> on all nodes of your cluster. <productname>&productname;</productname> includes all the required dependencies and
        extensions. 
      </para>
  <sect3 id="multimaster-setting-up-a-multi-master-cluster">
    <title>Setting up a Multi-Master Cluster</title>
      <para>After installing <productname>&productname;</productname> on all nodes, you need to
      configure the cluster with <filename>multimaster</filename>.</para>
      <para>Suppose
      you are setting up a cluster of three nodes, with
      <literal>node1</literal>, <literal>node2</literal>, and
      <literal>node3</literal> domain names. First, set up the database to be replicated, and make sure you have a user with superuser rights to perform replication:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              If you are starting from scratch, initialize a cluster,
              create an empty database <literal>mydb</literal> and a new
              user <literal>myuser</literal>, on each node of the cluster. For details, see <xref linkend="creating-cluster">.
            </para>
          </listitem>
          <listitem>
            <para>
              If you already have a database <literal>mydb</literal>
              running on <literal>node1</literal>, initialize
              new nodes from the working node using
              <application>pg_basebackup</application>. On behalf of <literal>myuser</literal>, run the following command on each node you are going to add:
            <programlisting>
pg_basebackup -D <replaceable>datadir</> -h node1 mydb
</programlisting>
where <replaceable>datadir</> is the directory containing the database cluster. This directory is specified at the cluster initialization stage, or set in the <envar>PGDATA</envar> environment variable.
</para>
            <para>
              For details on using <application>pg_basebackup</application>, see
              <xref linkend="app-pgbasebackup">.
            </para>
          </listitem>
        </itemizedlist>
      <para>Once the database is set up, complete the following steps on each
      cluster node:
    </para>
    <orderedlist>
      <listitem>
        <para>
          Modify the <filename>postgresql.conf</filename> configuration
          file, as follows:
        </para>
        <itemizedlist>
              <listitem><para>Add <literal>multimaster</literal> to the <varname>shared_preload_libraries</varname> variable:</para>
      <programlisting>
shared_preload_libraries = 'multimaster'
</programlisting>
<tip>
<para>If the <varname>shared_preload_libaries</varname> variable is already defined in <filename>postgresql.auto.conf</filename>, you will need to modify its value using the <literal>ALTER SYSTEM</literal> command. For details, see <xref linkend="config-setting-configuration-file">.
</para>
</tip>
</listitem>
          <listitem><para>Specify the transaction isolation level for your cluster. <filename>multimaster</filename> currently supports <link linkend="xact-read-committed">read committed</link> and <link linkend="xact-repeatable-read">repeatable read</link> isolation levels. 
          <programlisting>
default_transaction_isolation = 'read committed'</programlisting>
          </para>
          <important><para>Using <literal>repeatable read</literal> isolation level increases
          the probability of serialization failure at commit time. If such cases are not handled by your application, you are recommended to use <literal>read committed</literal> isolation level.</para></important>
          </listitem>
          <listitem>
            <para>
              Set up <productname>PostgreSQL</productname> parameters related to replication.
            </para>
            <programlisting>
wal_level = logical
max_connections = 100
max_prepared_transactions = 300
max_wal_senders = 10       # at least the number of nodes
max_replication_slots = 10 # at least the number of nodes
</programlisting>
            <para>
              You must change the replication level to
              <literal>logical</literal> as
              <filename>multimaster</filename> relies on logical
              replication. For a cluster of <replaceable>N</replaceable> nodes, enable at least <replaceable>N</replaceable> 
              WAL sender processes and replication slots. Since
              <filename>multimaster</filename> implicitly adds a
              <literal>PREPARE</literal> phase to each
              <literal>COMMIT</literal> transaction, make sure to set
              the number of prepared transactions to <replaceable>N</replaceable>*<varname>max_connections</varname>.
              Otherwise, prepared transactions may be queued.
            </para>
          </listitem>
          <listitem>
            <para>
              Make sure you have enough background workers allocated for
              each node:
            </para>
            <programlisting>
max_worker_processes = 250
</programlisting>
            <para>
              For example, for a three-node cluster with
              <literal>max_connections</literal> = 100,
              <filename>multimaster</filename> may need up to 206
              background workers at peak times: 200 workers for
              connections from the neighbor nodes, two workers for
              WAL sender processes, two workers for WAL receiver
              processes, and two workers for the arbiter sender and
              receiver processes. When setting this parameter, remember
              that other modules may also use background workers at the
              same time.
            </para>
          </listitem>
          <listitem>
            <para>
              Add <filename>multimaster</filename>-specific options:
            </para>
            <programlisting>
multimaster.max_nodes = 3  # cluster size
multimaster.node_id = 1    # the 1-based index of this node 
                           # in the cluster
multimaster.conn_strings = 'dbname=mydb user=myuser host=node1, dbname=mydb user=myuser host=node2, dbname=mydb user=myuser host=node3'
                           # comma-separated list 
                           # of connection strings 
                           # to neighbor nodes
</programlisting>
<para>The <literal>multimaster.max_nodes</literal> variable defines the maximum cluster size. If you plan to add new nodes to your cluster, the <literal>multimaster.max_nodes</literal> value should exceed the initial number of nodes. In this case, you can add new nodes without restarting <productname>&productname;</productname> until the specified number of nodes is reached.
</para>
<para>In most cases, three cluster nodes are enough to ensure high availability. Since the data on all cluster nodes is the same, you do not typically need more than five cluster nodes.
</para>
<para>If you would like to change the default connection settings for a cluster node, you can add other <link linkend="libpq-paramkeywords"> connection parameters</link> to the corresponding connection string in the <varname>multimaster.conn_strings</varname> variable. If you change the default port on which the arbiter process listens for connections, you must specify this port in the <literal>arbiter_port</literal> parameter. For details, see <xref linkend="multimaster-arbiter-port"> and <xref linkend="multimaster-conn-strings">.</para>
                <important><para>The
                <literal>multimaster.node_id</literal> variable takes natural
                numbers starting from 1, without any gaps in numbering.
                For example, for a cluster of five nodes, set node IDs
                to 1, 2, 3, 4, and 5. In the
                <literal>multimaster.conn_strings</literal> variable, make sure to
                list the nodes in the order of their IDs. The
                <literal>multimaster.conn_strings</literal> variable must be the
                same on all nodes.</para></important>

          </listitem>
        </itemizedlist>
        <para>
          Depending on your network environment and usage patterns, you
          may want to tune other <filename>multimaster</filename>
          parameters. For details, see
          <xref linkend="multimaster-tuning-configuration-parameters">.
        </para>
      </listitem>
      <listitem>
        <para>
          Modify the <filename>pg_hba.conf</filename> file to allow replication to each cluster node on behalf of <literal>myuser</literal>.
        </para>
      </listitem>
      <listitem>
        <para>
          Restart <productname>PostgreSQL</productname>:
        </para>
        <programlisting>
pg_ctl -D <replaceable>datadir</replaceable> -l <replaceable>pg.log</replaceable> start
</programlisting>
      </listitem>
    </orderedlist>
          <para>
          When <productname>&productname;</productname> is started on all nodes, connect to any node
          and create the <filename>multimaster</filename> extension:
          <programlisting>
psql -h node1
CREATE EXTENSION multimaster;</programlisting></para>
    <para>The <command>CREATE EXTENSION</command> query is replicated to all the cluster nodes.</para>
    <para>
      To ensure that <filename>multimaster</filename> is enabled, check
      the <structname>mtm.get_cluster_state()</structname> view:
    </para>
    <programlisting>
mtm.get_cluster_state();
</programlisting>
    <para>
      If <literal>liveNodes</literal> is equal to
      <literal>allNodes</literal>, your cluster is successfully
      configured and ready to use. </para>
      <para><emphasis role="strong">See Also</emphasis></para>
      <para><link linkend="multimaster-tuning-configuration-parameters">Tuning
      Configuration Parameters</link></para>
  </sect3>
    <sect3 id="multimaster-tuning-configuration-parameters">
    <title>Tuning Configuration Parameters</title>
    <para>
      While you can use <filename>multimaster</filename> in the default
      configuration, you may want to tune several parameters for faster
      failure detection or more reliable automatic recovery.
    </para>
    <sect4 id="multimaster-setting-timeout-for-failure-detection">
      <title>Setting Timeout for Failure Detection</title>
      <para>
        To check availability of the neighbor nodes,
        <filename>multimaster</filename> periodically sends heartbeat
        packets to all nodes. You can define the timeout for failure detection with the following variables:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            The <literal>multimaster.heartbeat_send_timeout</literal>
            variable defines the time interval between the
            heartbeats. By default, this variable is set to 1000ms.
          </para>
        </listitem>
        <listitem>
          <para>
            The <literal>multimaster.heartbeat_recv_timeout</literal>
            variable sets the timeout for the response. If no heartbeats are
            received during this time, the node is assumed to be
            disconnected and is excluded from the cluster. By default,
            this variable is set to 10000ms.
          </para>
        </listitem>
      </itemizedlist>
      <para>
        It's a good idea to set
        <literal>multimaster.heartbeat_send_timeout</literal> based on
        typical ping latencies between the nodes. Small recv/send ratio
        decreases the time of failure detection, but increases the
        probability of false-positive failure detection. When setting
        this parameter, take into account the typical packet loss ratio
        between your cluster nodes.
      </para>
    </sect4>
    <sect4 id="multimaster-configuring-automatic-recovery-parameters">
      <title>Configuring Automatic Recovery Parameters</title>
      <para>
        If a cluster node fails, <filename>multimaster</filename> can
        automatically restore it based on the WAL collected
        on other cluster nodes. To control the recovery settings, use the
        following variables:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            <varname>multimaster.min_recovery_lag</varname> &mdash; sets the
            minimal WAL lag between the node to be restored and the current cluster state. 
            By default,  <varname>multimaster.min_recovery_lag</varname> is set to 100KB.
            When the disconnected node is fast-forwarded up to the
            <varname>multimaster.min_recovery_lag</varname> threshold,
            <filename>multimaster</filename> stops all new commits to the
            alive nodes until the node fully catches up with the
            current state of the cluster. When the data is fully synchronized,
            the disconnected node is promoted to the online state, and
            the cluster resumes its work.
          </para>
        </listitem>
        <listitem>
          <para>
            <varname>multimaster.max_recovery_lag</varname> &mdash; sets the
            maximum size of WAL. Upon reaching the
            <varname>multimaster.max_recovery_lag</varname> threshold,
            WAL for the disconnected node is overwritten. At this
            point, automatic recovery is no longer possible. In this case, you can <link linkend="multimaster-restoring-a-node-manually">restore the node manually</link> by cloning the data from one of the alive nodes using <application>pg_basebackup</application>.
          </para>
        </listitem>
      </itemizedlist>
      <para>
        By default, <varname>multimaster.max_recovery_lag</varname> is
        set to 100MB. Setting
        <varname>multimaster.max_recovery_lag</varname> to a larger
        value increases the timeframe for automatic recovery, but
        requires more disk space for WAL collection.
      </para>
      <para><emphasis role="strong">See Also</emphasis></para>
      <para><link linkend="multimaster-guc-variables">GUC Variables</link></para>
    </sect4>
  </sect3>
  </sect2>
  <sect2 id="multimaster-administration"><title>Multi-Master Cluster Administration</title>
  <itemizedlist>
    <listitem>
      <para>
        <link linkend="multimaster-monitoring-cluster-status">Monitoring the Cluster Status</link>
      </para>
    </listitem><listitem>
      <para>
        <link linkend="multimaster-adding-new-nodes-to-the-cluster">Adding New Nodes
        to the Cluster</link>
      </para>
    </listitem>
    <listitem>
      <para>
        <link linkend="multimaster-excluding-nodes-from-the-cluster">Excluding Nodes
        from the Cluster</link>
      </para>
    </listitem>
    <listitem>
      <para>
        <link linkend="multimaster-restoring-a-node-manually">Restoring a Cluster Node</link>
      </para>
    </listitem>
  </itemizedlist>
  <sect3 id="multimaster-monitoring-cluster-status">
    <title>Monitoring Cluster Status</title>
    <para>
      <filename>multimaster</filename> provides several views to check the
      current cluster state.
    </para>
    <para>
      To check node-specific information, use <literal>mtm.get_nodes_state()</literal>:
    </para>
    <programlisting>
mtm.get_nodes_state();
</programlisting>
      <para>To check the status of the whole cluster, use the
      <literal>mtm.get_cluster_state()</literal> view:
    </para>
    <programlisting>
mtm.get_cluster_state();
</programlisting>
      <para>For details on all the returned information, see <xref linkend="multimaster-functions">.
    </para>
  </sect3>
  <sect3 id="multimaster-adding-new-nodes-to-the-cluster">
    <title>Adding New Nodes to the Cluster</title>
    <para>With the <filename>multimaster</filename> extension, you can add or drop cluster nodes without
      stopping the database service.
    </para>
    <para>
      To add a new node, you need to change the cluster
      configuration on alive nodes, load all the data to the new node using
      <application>pg_basebackup</application>, and start the node.
    </para>
    <para>
      Suppose we have a working cluster of three nodes, with
      <literal>node1</literal>, <literal>node2</literal>, and
      <literal>node3</literal> domain names. To add
      <literal>node4</literal>, follow these steps:
    </para>
    <orderedlist>
      <listitem>
        <para>Check whether the current number of cluster nodes has reached the value specified in the
        <varname>multimaster.max_nodes</varname> variable. If this value is reached, increase
        the <varname>multimaster.max_nodes</varname> value on each node and restart all nodes.
        You can restart the nodes one by one, without stopping the database.
        If the maximum number of nodes is not reached, proceed to the next step. 
        </para>
      </listitem>
      <listitem>
        <para>
          Figure out the required connection string to
          access the new node. For example, for the database
          <literal>mydb</literal>, user <literal>myuser</literal>, and
          the new node <literal>node4</literal>, the connection string
          can be <literal>&quot;dbname=mydb user=myuser host=node4&quot;</literal>.
          For details, see <xref linkend="multimaster-conn-strings">.
        </para>
      </listitem>
      <listitem>
        <para>
          In <literal>psql</literal> connected to any alive node, run:
        </para>
        <programlisting>
mtm.add_node('dbname=mydb user=myuser host=node4');
</programlisting>
        <para>
          This command changes the cluster configuration on all nodes
          and starts replication slots for the new node.
        </para>
      </listitem>
      <listitem>
        <para>
          Connect to the new node and clone all the data from one of the alive nodes to the new node:
        </para>
        <programlisting>
pg_basebackup -D <replaceable>datadir</replaceable> -h node1 -x
</programlisting>
        <para>
          <application>pg_basebackup</application> copies the entire data
          directory from <literal>node1</literal>, together with
          configuration settings.
        </para>
      </listitem>
      <listitem>
        <para>
          Update <filename>postgresql.conf</filename> settings on
          <literal>node4</literal>:
        </para>
        <programlisting>
multimaster.max_nodes = 4
multimaster.node_id = 4
multimaster.conn_strings = 'dbname=mydb user=myuser host=node1, dbname=mydb user=myuser host=node2,
                            dbname=mydb user=myuser host=node3, dbname=mydb user=myuser host=node4'
</programlisting>
      </listitem>
      <listitem>
        <para>
          Start <productname>PostgreSQL</productname> on the new node:
        </para>
        <programlisting>
pg_ctl -D <replaceable>datadir</replaceable> -l <replaceable>pg.log</replaceable> start
</programlisting>
        <para>
          All the cluster nodes get locked for write transactions until the new node retrieves all the updates that happened after you started making a base backup.
          When data recovery is complete, <filename>multimaster</filename> promotes the new node to the online state and includes it into the replication scheme.
        </para>
      </listitem>
      </orderedlist>
      <para>To ensure that the new configuration is loaded in the case of <productname>PostgreSQL</productname> restart, update configuration settings on all the cluster nodes: </para>
      <orderedlist>
      <listitem>
        <para>
          Change <literal>multimaster.conn_strings</literal> and <literal>multimaster.max_nodes</literal> to include the new node.
        </para>
      </listitem>
      <listitem>
        <para>
          Make sure the <filename>pg_hba.conf</filename> file allows
          replication to the new node.
        </para>
      </listitem>
      <listitem>
        <para>
          Restart all cluster nodes.
        </para>
      </listitem>
    </orderedlist>
    <para>
      <emphasis role="strong">See Also</emphasis></para>
      <para><link linkend="multimaster-setting-up-a-multi-master-cluster">Setting up a
      Multi-Master Cluster</link></para>
      <para><link linkend="multimaster-monitoring-cluster-status">Monitoring Cluster
      Status</link></para>
  </sect3>
  <sect3 id="multimaster-excluding-nodes-from-the-cluster">
    <title>Excluding Nodes from the Cluster</title>
    <para>
      To exclude a node from the cluster, use the
      <literal>mtm.stop_node()</literal> function. For example, to
      exclude node 3, run the following command on any other node:
    </para>
    <programlisting>
SELECT mtm.stop_node(3);
</programlisting>
    <para>
      This disables replication slots for node 3 on all cluster nodes and stops replication to
      this node.
    </para>
    <para>
      If you simply shutdown a node, it will be excluded
      from the cluster as well. However, all transactions in the cluster
      will be frozen until other nodes detect the offline state of the node.
      This time interval is defined by the <literal>multimaster.heartbeat_recv_timeout</literal> parameter.
    </para>
  </sect3>
  <sect3 id="multimaster-restoring-a-node-manually">
    <title>Restoring a Cluster Node</title>
    <para>
      The <filename>multimaster</filename> extension can <link linkend="multimaster-failure-detection-and-recovery">automatically restore</link> a failed node if the WAL is available for the time when the node was disconnected from the cluster. However, if the data updates on the alive nodes exceed the allowed WAL size specified in the <literal>multimaster.max_recovery_lag</literal> variable. In this case, you can manually restore the failed node.
      </para>
    <para>
      Suppose <literal>node2</literal> got disconnected from your three-node cluster and needs to be manually restored. The typical workflow is as follows:
    </para>
    <orderedlist>
      <listitem>
        <para>
          In <literal>psql</literal> connected to any alive node, create a new replication slot for the disconnected node with the following command:
        </para>
        <programlisting>
mtm.recover_node(2);
</programlisting>
<para>where 2 is the ID of the disconnected node specified in the <varname>multimaster.node_id</varname> variable.</para>
      </listitem>
      <listitem>
        <para>
          Connect to <literal>node2</literal> and clone all the data from one of the alive nodes:
        </para>
        <programlisting>
pg_basebackup -D <replaceable>datadir</replaceable> -h node1 -x
</programlisting>
        <para>
          <application>pg_basebackup</application> copies the entire data
          directory from <literal>node1</literal>, together with
          configuration settings.
        </para>
      </listitem>
      <listitem>
        <para>
          Start <productname>PostgreSQL</productname> on the restored node:
        </para>
        <programlisting>
pg_ctl -D <replaceable>datadir</replaceable> -l <replaceable>pg.log</replaceable> start
</programlisting>
        <para>
          All the cluster nodes get locked for write transactions until the restored node retrieves all the updates that happened after you started making a base backup.
          When data recovery is complete, <filename>multimaster</filename> promotes the new node to the online state and includes it into the replication scheme.
        </para>
      </listitem>
    </orderedlist>
    <para>
      <emphasis role="strong">See Also</emphasis></para>
      <para><link linkend="multimaster-failure-detection-and-recovery">Failure Detection and Recovery</link></para>
  </sect3>
  </sect2>
  <sect2 id="multimaster-reference"><title>Reference</title>
<sect3 id="multimaster-guc-variables">
  <title>GUC Variables</title>
<variablelist>
  <varlistentry><term><varname>multimaster.node_id</varname>
  <indexterm><primary><varname>multimaster.node_id</varname></primary></indexterm></term><listitem><para>
    Node ID &mdash; a unique natural
    number identifying the node of a multi-master cluster. You must
    start node numbering from 1. There must be no gaps in numbering.
    For example, for a cluster of five nodes, set node IDs to 1, 2, 3,
    4, and 5.
  </para></listitem></varlistentry>
  <varlistentry id="multimaster-conn-strings"><term><varname>multimaster.conn_strings</varname><indexterm><primary><varname>multimaster.conn_strings</varname></primary></indexterm></term><listitem><para>
    Connection strings for each node of a multi-master cluster, separated by commas. 
    The <varname>multimaster.conn_strings</varname> parameter must be identical on all nodes.
    Each connection string must include the name of the database to replicate
    and the cluster node domain name. For example, 'dbname=mydb
    host=node1, dbname=mydb host=node2, dbname=mydb host=node3'.
    Optionally, you can add other <link linkend="libpq-paramkeywords">connection parameters</link> to change the default connection settings.
    Connection strings must appear in the order of the node IDs
    specified in the <varname>multimaster.node_id</varname> variable.
    Connection string for the i-th node must be on the i-th position.
    If you specify a custom port in the <varname>multimaster.arbiter_port</varname>, 
    you must provide this value in the <literal>arbiter_port</literal>
    parameter in the connection string for the corresponding node.
  </para></listitem></varlistentry>
  <varlistentry><term><varname>multimaster.max_nodes</varname><indexterm><primary><varname>multimaster.max_nodes</varname></primary></indexterm></term><listitem><para>
    The maximum number of nodes allowed in the cluster. If you plan to add new nodes to your cluster, the <literal>multimaster.max_nodes</literal> value should exceed the initial number of nodes. In this case, you can add new nodes without restarting <productname>&productname;</productname> until the specified number of nodes is reached. In most cases, three cluster nodes are enough to ensure high availability. Since the data on all cluster nodes is the same, you do not typically need more than five cluster nodes. The maximum possible number of nodes is limited to 64.</para>
    <para>Default: the number of nodes specified in the <varname>multimaster.conn_strings</varname> variable
  </para></listitem></varlistentry>
  <varlistentry id="multimaster-arbiter-port">
  <term><varname>multimaster.arbiter_port</varname><indexterm><primary><varname>multimaster.arbiter_port</varname></primary></indexterm></term><listitem><para>
    Port for the arbiter process to listen on. If you change the default value, you must specify this value in the <literal>arbiter_port</literal>
    parameter in the connection string for the corresponding node.</para>
    <para>Default: 5433
  </para></listitem></varlistentry>
  <varlistentry><term><varname>multimaster.heartbeat_send_timeout</varname><indexterm><primary><varname>multimaster.heartbeat_send_timeout</varname></primary></indexterm></term><listitem><para>
    Time interval
    between heartbeat messages, in milliseconds. An arbiter process
    broadcasts heartbeat messages to all nodes to detect connection
    problems. </para><para>Default: 1000
  </para></listitem></varlistentry>
  <varlistentry><term><varname>multimaster.heartbeat_recv_timeout</varname><indexterm><primary><varname>multimaster.heartbeat_recv_timeout</varname></primary></indexterm></term><listitem><para>
    Timeout, in
    milliseconds. If no heartbeat message is received from the node
    within this timeframe, the node is excluded from the cluster.
    </para><para>Default: 10000
  </para></listitem></varlistentry>
  <varlistentry><term><varname>multimaster.min_recovery_lag</varname><indexterm><primary><varname>multimaster.min_recovery_lag</varname></primary></indexterm></term><listitem><para>
    Minimal WAL lag
    between the node to be restored and the current cluster state, in
    bytes. When this threshold is reached during node recovery, the
    cluster is locked for write transactions until the recovery is
    complete. </para><para>Default: 100000
  </para></listitem></varlistentry>
  <varlistentry><term><varname>multimaster.max_recovery_lag</varname><indexterm><primary><varname>multimaster.max_recovery_lag</varname></primary></indexterm></term><listitem><para>
    Maximal WAL lag
    size, in bytes. When a node is disconnected from the cluster, other
    nodes copy WALs for all new transactions into the replication slot of
    this node. Upon reaching the
    <varname>multimaster.max_recovery_lag</varname> value, the
    replication slot for the disconnected node is dropped to avoid
    overflow. At this point, automatic recovery of the node is no longer
    possible. In this case, you can restore the node manually by cloning
    the data from one of the alive nodes using
    <application>pg_basebackup</application> or a similar tool. If you set this
    variable to zero, replication slot will not be dropped. </para><para>Default:
    100000000
  </para></listitem></varlistentry>
  <varlistentry><term><varname>multimaster.ignore_tables_without_pk</varname><indexterm><primary><varname>multimaster.ignore_tables_without_pk</varname></primary></indexterm></term><listitem><para>
    Boolean.
    This variable enables/disables replication of tables without primary
    keys. By default, such replication is
    disabled because of the logical replication restrictions. To enable
    replication of tables without primary keys, you can set this variable to <literal>false</literal>. However, take into
    account that <varname>multimaster</varname> does not allow update
    operations on such tables. </para><para>Default: true
  </para></listitem></varlistentry>
  <varlistentry><term><varname>multimaster.cluster_name</varname><indexterm><primary><varname>multimaster.cluster_name</varname></primary></indexterm></term><listitem><para>
    Name of the cluster. If
    you define this variable when setting up the cluster, <filename>multimaster</filename> checks that
    the cluster name is the same for all the cluster nodes.
  </para></listitem></varlistentry>
  <varlistentry>
    <term><varname>multimaster.break_connection</varname>
      <indexterm><primary><varname>multimaster.break_connection</varname></primary>
      </indexterm>
    </term>
    <listitem>
      <para>Break connection with clients connected to the node if this node disconnects
      from the cluster. If this variable is set to <literal>false</literal>, the client stays
      connected to the node but receives an error that the node is in minority.
      </para>
      <para>Default: <literal>false</literal>
      </para>
    </listitem>
  </varlistentry>
  <varlistentry>
    <term><varname>multimaster.major_node</varname>
      <indexterm><primary><varname>multimaster.major_node</varname></primary>
      </indexterm>
    </term>
    <listitem>
      <para>Node with this flag continues working even if it cannot access the majority of other nodes.
      This is needed to break the symmetry if there is an even number of alive nodes in the cluster.
      For example, a cluster with three nodes continues working if one of the nodes has crashed.
      If connection between the remaining nodes is lost, the node with <varname>multimaster.major_node</varname> = <literal>true</literal> will continue working.
      </para>
      <important>
        <para>This parameter should be used with caution. Only one node in the cluster
        can have this parameter set to true. When set to <literal>true</literal> on several
        nodes, this parameter can cause the split-brain problem.
        </para>
      </important>
    </listitem>
  </varlistentry>
  <varlistentry>
    <term><varname>multimaster.max_workers</varname>
      <indexterm><primary><varname>multimaster.max_workers</varname></primary>
      </indexterm>
    </term>
    <listitem>
      <para>The maximum number of <literal>walreceiver</literal> workers on this server.
      </para>
      <important>
      <para>This parameter should be used with caution. If the number of simultaneous transactions
      in the whole cluster is bigger than the provided value, it can lead to undetected deadlocks.
      </para>
      </important>
    </listitem>
  </varlistentry>
  <varlistentry>
    <term><varname>multimaster.trans_spill_threshold</varname>
      <indexterm><primary><varname>multimaster.trans_spill_threshold</varname></primary>
      </indexterm>
    </term>
    <listitem>
      <para>The maximal size of transaction, in MB. When this threshold is reached, the transaction is written to the disk.
      </para>
      <para>Default: 100
      </para>
    </listitem>
  </varlistentry>
</variablelist>
</sect3>
  <sect3 id="multimaster-functions"><title>Functions</title>
  <variablelist>
  <varlistentry>
     <term>
      <function>mtm.get_nodes_state()</function>
      <indexterm>
       <primary><function>mtm.get_nodes_state()</></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Shows the status of all nodes in the cluster. Returns a tuple of the following values:
      </para>
      <para>
       <itemizedlist>
          <listitem>
            <para>
              <parameter>id</parameter>, <type>integer</type>
            </para>
            <para>Node ID.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>enabled</parameter>, <type>boolean</type>
            </para>
            <para>Shows whether the node is excluded from the cluster. The node can only be disabled if responses to heartbeats are not received within the <varname>heartbeat_recv_timeout</> time interval. When the node starts responding to heartbeats, <filename>multimaster</filename> can automatically restore the node and switch it back to the enabled state.
            Automatic recovery is only possible if the replication slot is still active. Otherwise, you can <link linkend="multimaster-restoring-a-node-manually">restore the node manually</link>.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>connected</parameter>, <type>boolean</type>
            </para>
            <para>
            Shows whether the node is connected to the WAL sender.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>slot_active</parameter>, <type>boolean</type>
            </para>
            <para>Shows whether the node has an active replication slot. For a disabled node, the slot remains active until the <varname>max_recovery_lag</varname> value is reached.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>stopped</parameter>, <type>boolean</type>
            </para>
            <para>Shows whether replication to this node was stopped by the <function>mtm.stop_node()</function> function. A stopped node acts as a disabled one, but cannot be automatically recovered. Call <function>mtm.recover_node()</function> to re-enable such a node.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>catchUp</parameter>, <type>boolean</type>
            </para>
            <para>During the node recovery, shows whether the data is recovered up to the <varname>min_recovery_lag</varname> value.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>slotLag</parameter>, <type>bigint</type>
            </para>
            <para>The size of WAL data that the replication slot holds for a disabled/stopped node. The slot is dropped when <literal>slotLag</literal> reaches the <literal>max_recovery_lag</literal> value.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>avgTransDelay</parameter>, <type>bigint</type>
            </para>
            <para>An average commit delay caused by this node, in microseconds.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>lastStatusChange</parameter>, <type>timestamp</type>
            </para>
            <para>Last time when the node changed its status (enabled/disabled).</para>
          </listitem>
          <listitem>
            <para>
              <parameter>oldestSnapshot</parameter>, <type>bigint</type>
            </para>
            <para>The oldest global snapshot existing on this node.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>SenderPid</parameter>, <type>integer</type>
            </para>
            <para>Process ID of the WAL sender.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>SenderStartTime</parameter>, <type>timestamp</type>
            </para>
            <para>WAL sender start time.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>ReceiverPid</parameter>, <type>integer</type>
            </para>
            <para>Process ID of the WAL receiver.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>ReceiverStartTime</parameter>, <type>timestamp</type>
            </para>
            <para>WAL receiver start time.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>connStr</parameter>, <type>text</type>
            </para>
            <para>Connection string to this node.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>connectivityMask</parameter>, <type>bigint</type>
            </para>
            <para>Bitmask representing connectivity to neighbor nodes. Each bit represents a connection to node.</para>
          </listitem>
          <listitem>
          <para><parameter>nHeartbeats</parameter>, <type>integer</type></para>
          <para>The number of heartbeat responses received from this node.</para>
          </listitem>
        </itemizedlist>
      </para>
     </listitem>
    </varlistentry>

    <varlistentry>
     <term>
      <function>mtm.collect_cluster_state()</function>
      <indexterm>
       <primary><function>mtm.collect_cluster_state</></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Collects the data returned by the <function>mtm.get_cluster_state()</function> function from all available nodes. For this function to work, in addition to replication connections, <filename>pg_hba.conf</filename> must allow ordinary connections to the node with the specified connection string.
      </para>
     </listitem>
    </varlistentry>

        <varlistentry>
     <term>
      <function>mtm.get_cluster_state()</function>
      <indexterm>
       <primary><function>mtm.get_cluster_state()</></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Shows the status of the <filename>multimaster</filename> extension. Returns a tuple of the following values:
      </para>
       <itemizedlist>
          <listitem>
            <para>
              <parameter>status</parameter>, <type>text</type>
            </para>
            <para>Node status. Possible values are: <literal>Initialization</literal>, <literal>Offline</literal>, <literal>Connected</literal>, <literal>Online</literal>, <literal>Recovery</literal>, <literal>Recovered</literal>, <literal>InMinor</literal>, <literal>OutOfService</literal>.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>disabledNodeMask</parameter>, <type>bigint</type>
            </para>
            <para>Bitmask of disabled nodes.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>disconnectedNodeMask</parameter>, <type>bigint</type>
            </para>
            <para>Bitmask of disconnected nodes.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>catchUpNodeMask</parameter>, <type>bigint</type>
            </para>
            <para>Bitmask of nodes that completed the recovery.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>liveNodes</parameter>, <type>integer</type>
            </para>
            <para>Number of enabled nodes.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>allNodes</parameter>, <type>integer</type>
            </para>
            <para>Number of nodes in the cluster. The majority of alive nodes is calculated based on this parameter.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>nActiveQueries</parameter>, <type>integer</type>
            </para>
            <para>Number of queries being currently processed on this node.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>nPendingQueries</parameter>, <type>integer</type>
            </para>
            <para>Number of queries waiting for execution on this node.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>queueSize</parameter>, <type>bigint</type>
            </para>
            <para>Size of the pending query queue, in bytes.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>transCount</parameter>, <type>bigint</type>
            </para>
            <para>The total number of replicated transactions processed by this node.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>timeShift</parameter>, <type>bigint</type>
            </para>
            <para>Global snapshot shift caused by unsynchronized clocks on nodes, in microseconds.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>recoverySlot</parameter>, <type>integer</type>
            </para>
            <para>The node from which a failed node gets data updates during automatic recovery.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>xidHashSize</parameter>, <type>bigint</type>
            </para>
            <para>Size of xid2state hash.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>gidHashSize</parameter>, <type>bigint</type>
            </para>
            <para>Size of gid2state hash.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>oldestXid</parameter>, <type>bigint</type>
            </para>
            <para>The oldest transaction ID on this node.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>configChanges</parameter>, <type>integer</type>
            </para>
            <para>Number of state changes (enabled/disabled) since the last reboot.</para>
          </listitem>
          <listitem>
            <para>
              <parameter>stalledNodeMask</parameter>, <type>bigint</type>
            </para>
            <para>Bitmask of nodes for which replication slots were dropped.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>stoppedNodeMask</parameter>, <type>bigint</type>
            </para>
            <para>Bitmask of nodes that were stopped by <function>mtm.stop_node()</function>.
            </para>
          </listitem>
          <listitem>
            <para>
              <parameter>lastStatusChange</parameter>, <type>timestamp</type>
            </para>
            <para>Timestamp of the last state change.
            </para>
          </listitem>
        </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>
      <function>mtm.add_node(<parameter>conn_str</parameter> <type>text</type>)</function>
      <indexterm>
       <primary><function>mtm.add_node</></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Adds a new node to the cluster.
      </para>
      <para>
       Arguments:
       <itemizedlist>
        <listitem>
         <para>
         <parameter>conn_str</parameter> &mdash; connection string for the
              new node. For example, for the database
              <literal>mydb</literal>, user <literal>myuser</literal>,
              and the new node <literal>node4</literal>, the connection
              string is
              <literal>&quot;dbname=mydb user=myuser host=node4&quot;</literal>.</para>
         <para>Type: <literal>text</literal></para>
        </listitem>
        </itemizedlist>
      </para>
      <para>
      </para>
     </listitem>
    </varlistentry>
        <varlistentry>
     <term>
      <function>mtm.drop_node(<parameter>node</parameter> <type>integer</type>, <parameter>drop_slot</parameter> <type>bool</type> default false)</function>
      <indexterm>
       <primary><function>mtm.drop_node</></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Excludes a node from the cluster.
      </para>
      <para>
       Arguments:
       <itemizedlist>
        <listitem>
         <para>
         <parameter>node</parameter> &mdash; ID of the node to be dropped
              that you specified in the
              <varname>multimaster.node_id</varname> variable.</para>
              <para> Type:
              <literal>integer</literal></para>
        </listitem>
        <listitem>
         <para>
         <parameter>drop_slot</parameter> &mdash; Optional. Defines whether
              the replication slot should be dropped together with the
              node. Set this option to <literal>true</literal> if you do not plan to
              restore the node in the future. </para>
              <para>Type: <literal>boolean</literal></para>
              <para>Default: <literal>false</literal></para>
        </listitem>
        </itemizedlist>
      </para>
      <para>
      </para>
     </listitem>
    </varlistentry>
        <varlistentry>
     <term>
      <function>mtm.recover_node(<parameter>node</parameter> <type>integer</type>)</function>
      <indexterm>
       <primary><function>mtm.recover_node</></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Creates a
          replication slot for the node that was previously dropped
          together with its slot.
      </para>
      <para>
       Arguments:
       <itemizedlist>
        <listitem>
         <para>
         <parameter>node</parameter> &mdash; ID of the node to be restored.</para>
        </listitem>
        </itemizedlist>
      </para>
     </listitem>
    </varlistentry>
        <varlistentry>
     <term>
      <function>mtm.make_table_local(<parameter>relation</parameter> <type>regclass</type>)</function>
      <indexterm>
       <primary><function>mtm.make_table_local</></primary>
      </indexterm>
     </term>
     <listitem>
      <para>Stops replication for the specified table.
      </para>
      <para>
       Arguments:
       <itemizedlist>
        <listitem>
         <para>
         <parameter>relation</parameter> &mdash; The table you would like to
              exclude from the replication scheme.</para>
              <para>Type: <literal>regclass</literal></para>
        </listitem>
        </itemizedlist>
      </para>
      <para>
      </para>
     </listitem>
    </varlistentry>
    </variablelist>
  </sect3>
  </sect2>
  <sect2 id="multimaster-compatibility">
    <title>Compatibility</title>
    <para>
      The <filename>multimaster</filename> extension currently passes 162
      of 166 <productname>PostgreSQL</productname> regression tests. We are working right now on
      providing full compatibility with the standard <productname>PostgreSQL</productname>.
    </para>
  </sect2>
  <sect2 id="multimaster-authors">
    <title>Authors</title>
    <para>
      Postgres Professional, Moscow, Russia.
    </para>
    <sect3 id="multimaster-credits">
      <title>Credits</title>
      <para>
        The replication mechanism is based on logical decoding and an
        earlier version of the <filename>pglogical</filename> extension
        provided for community by the 2ndQuadrant team.
      </para>
      <para>The three-phase E3PC commit protocol is based on the following works:
      <itemizedlist>
      <listitem>
      <para>Idit Keidar, Danny Dolev. <ulink url="http://dx.doi.org/10.1006/jcss.1998.1566"><citetitle>Increasing the Resilience of Distributed and Replicated Database Systems.</citetitle></ulink>
      </para>
      </listitem>
      <listitem>
      <para>Tim Kempster, Colin Stirling, Peter Thanisch. <ulink url="http://dx.doi.org/10.1007/BFb0056487"><citetitle>A More Committed Quorum-Based Three Phase Commit Protocol</citetitle></ulink>.
      </para>
      </listitem>
      </itemizedlist>
    </para>
    </sect3>
  </sect2>
</sect1>
